%! TeX program = lualatex

\documentclass[a4paper]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{geometry}[margins=1in]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[square, numbers, sort&compress]{natbib}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{relsize}
\usepackage{todonotes}
\usepackage[ruled, vlined]{algorithm2e}

\bibliographystyle{abbrvnat}

\title{\texttt{bigrig}: A historical biogeography range simulator for the
	DEC[+J]}
\author{Ben Bettisworth}

\newcommand{\CountFull}[1]{|#1|_\text{full}}
\newcommand{\CountEmpty}[1]{|#1|_\text{empty}}
\newcommand{\bigrig}{\texttt{bigrig}}
\newcommand{\decj}{DEC[+J]}
\newcommand{\rand}[2]{#1 \land #2}
\newcommand{\ror}[2]{#1 \lor #2}
\newcommand{\rneg}[1]{\neg #1}
\newcommand{\rxor}[2]{#1 \oplus #2}
\newcommand{\rLshift}[2]{#1 \ll #2}
\newcommand{\rRshift}[2]{#1 \gg #2}

\begin{document}

\maketitle

\section{Introduction}

Verifying the correctness of phylogenetics software constitutes a perpetual challenge for tool developers
\cite{darribaStateSoftwareEvolutionary2018, mendesHowValidateBayesian2025, carver_software_2007,
bettisworth_lagrange-ng_2023}. 
The true phylogenetic tree\textemdash the series of speciation events originating from a common ancestor which explain an
evolutionary history\textemdash is almost always unobservable. 
It is, generally,impossible to verify the results generated by software tools with a true, empirical reference phylogeny (but see \cite{hillis_experimental_1992} for one of the few notable exceptions.) 
As it currently stands, datasets for which the truth is known are exceptionally rare.
Therefore, researchers cannot rely solely on these occasional known phylogenies to verify tool correctness.

Nonetheless, software verification remains vital, as without verification it is challenging to discriminate
between a surprising result and an erroneous result which is the result of a software error.
Tool developers must therefore rely on artificial data to verify the correctness of
their programs \cite{mendesHowValidateBayesian2025, ly-trong_alisim_2022, fletcher_indelible_2009}.
Generally, these artificial data are generated by assuming some statistical model of evolution, and subsequently generating these 
data for a given set of model parameters.
Data generated with this approach are unrealistic as they fail to capture the complexities of real biological systems 
\cite{trost_simulations_2024} and the models which are tractable rely upon a myriad of non-realistic assumptions (e.g., that alignment 
sites evolve independently.)
Nonetheless, the use of artificial data is still ubiquitous for software verification as there exists no viable alternative. 
Importantly, software simulators are subject to the same concerns as inference software.
Below, we discuss the methods which we can use to increase our confidence that the implementation of our simulator is correct. 

The field of historical biogeography faces an analogous challenge.
A common goal in historical biogeography is to infer the ancestral ranges of species along a known phylogenetic tree
\citep{varela_phylogeny_2019, baker_global_2013, vicente_and_2017}.
However, similar to the true phylogenetic tree (see above), these ancestral ranges generally can not be directly observed.
Some ancestral ranges can be estimated from the fossil record \cite{mclachlan_reconstructing_2004}, but there can be multiple
complications which can limit our knowledge about the true ancestral ranges.
These complications include, but are not limited to: gaps in the fossil record; the study system includes soft bodied
organisms; or a geographical distribution which is unsuited for fossilization \cite{kidwell_quality_2002}.
Overall, there does not exist a reliable source of known ancestral ranges and associated phylogenetic trees. 
Therefore one must turn to simulations in order to verify the correctness of respective software tools.
To date, biogeographical data simulations under the DEC[+J] model have often been implemented in an ad hoc manner,
with individual tools implementing their own simulation framework \cite{matzke_statistical_2022,
bettisworth_lagrange-ng_2023}, even though tools which can generate simulated data do exist \cite{hohna_revbayes_2016}.
Sometimes, these simulations utilize the same code base for the inference of ancestral ranges and corresponding model parameters.
In such cases it is more likely that a software error in the model inference will also corrupt simulation results.

Instead of implementing ad hoc simulations for each software project, developing an independent stand-alone 
simulation software for a model (or class of models) reduces the aforementioned error margin.
Separating simulation from inference code, as mentioned above, reduces the chances of integrated bugs in both the simulation
and the inference, which might improve apparent performance at the expense of actual performance.
This is to say, if there is an error in a shared region of code between inference and simulation, the error could be
masked, as the simulation step (erroneously) agrees with the inference step.
Furthermore, it is substantially easier to introspect results of a well implemented simulator.
Specifically, events which are typically marginalized over in likelihood models are instead explicitly simulated, and can therefore be recorded for later analysis.
For example, a state change along the branch of a phylogenetic tree can be recorded in a log file.
This explicit logging of results ensures the transparency of results, which in turn guarantees that the resulting data are
explainable.
This also allows to more rapidly identify bugs \textit{in the simulation tool}, as erroneous results will have erroneous explanations.
Additional records of intermediate states and marginalized parameters will also enable future tools to implement and test inference of
these parameters without any further modification to the simulation tool.

Finally, it is critical that tools which implement inferences under a specific model are assessed using a common (simulation) standard.
By using a common standard to evaluate tools one can ensure that improved performance (w.r.t., computational performance or inference accuracy) is not merely a consequence of the evaluation process.
%In addition, a common simulation tool drastically reduces the amount of duplicated work for tool developers.
A common simulator also decreases the amount of work required to implement a new inference tool, as a fast and reliable method of generating testing data can be utilized by developers in the context of iterative development cycles.
This will have the (eventual) result of increasing the number of tools available, and will increase the overall quality of tooling.

In this paper we present \bigrig{}, a simulator for the \decj{} \todo{And possibly a limited version of GeoSSE} model
which complies with all of the aforementioned criteria for simulators.
We explain the the \decj{} model in detail in Section~\ref{sec:model}.
Given a set of \decj{} parameters and a phylogenetic tree, \bigrig{} will generate a range dataset  for the taxa at the
tips of the tree.
In addition, ranges for inner nodes, cladogenesis events, and state transitions along branches will be generated.
After generation, \bigrig{} will log the results in a variety of practical file formats (YAML, JSON, or CSV), as well as output
the results in phylip format, and an annotated tree in Newick format.

Furthermore, we show that \bigrig{} is both highly reliable and extremely computationally efficient in regards to runtime and memory usage.
We show that \bigrig{} is reliable via two distinct approaches.
First, we derive expected distributions for the results of fundamental model events, and perform statistical
tests to ensure that the results are within 0.0001 with 99.999\% of type 1 or type 2 error.
Second, we perform the simulation via a completely independent implementation that uses a slower, yet also simpler-to-implement
method. 
Subsequently, we conduct statistical tests to verify that the results of the two independent implementations agree.
Finally, we show that \bigrig{} is computationally efficient. 
It can generate datasets for trees with tens of thousands of tips and 63 regions in under 0.5 \todo{check the time to be sure} seconds on a mid-range laptop.

\todo[inline]{Simpify this section. Specifically, I need to explain why other simulators aren't typically used, and the
difficulty of using RevBayes.}

\section{Background}

In the context of historical biogeography, an individual geographic sector where a taxon may be present is refered to as a \textit{region}.
An \textit{area} comprises at least two non-overlapping regions, and all regions together represent the entire area under study.
When not otherwise stated, we use the term ``area'' to refer to the area containing \textit{all} regions.
A \textit{range} describes the absence or presence of a specific taxon for every region in an area.
We denote the total number of regions for a range \( r \) as \( |r| \).
Each region can either be occupied, in which case we say it is full, or it can be unoccupied, in which case we say it is
empty.
We write the number of full regions a range has as \( \CountFull{r} \), and the number of empty regions as \(
\CountEmpty{r} \).
We denote the value of the $i$th region of a range as $r_i$.
In practice and for simplicity's sake, we represent ranges as binary strings, with $1$ indicating a full region, and $0$
denoting an empty region.
\todo{Maybe an example?}

For the regions $r$ and $s$ we denote the \textit{bitwise and} operation as $\rand{r}{s}$, the \textit{bitwise or} as $\ror{r}{s}$,
the \textit{bitwise exclusive or} as $\rxor{r}{s}$, and the \textit{bitwise negation} as $\rneg{r}$.
Here, \textit{bitwise} indicates that the operation is executed independently on each character in the string (that is,
each region). 
For example the \textit{bitwise and} is defined as detail $r_i \land s_i = t_i$ where $0 \leq i \leq |r|$, $\texttt{1}
\land \texttt{1} = \texttt{1}$ and is equal to \texttt{0} otherwise.
Additionally, we denote the non-rotating \textit{right shift} as $\rRshift{r}{n}$, and the non-rotating \textit{left shift} as
$\rLshift{r}{n}$, for some positive integer $n$.

These operations are of a particular interest as they can be efficiently computed via fundamental CPU instructions.
Consequently, computing the results of these operations often requires less than a single CPU cycle\footnotemark
\citep{Abel19a}.
As such, computation of these operations (and functions which are composed of these operations) is extremely efficient.

\footnotetext{This is due to the capabilities of modern CPUs to potentially execute more than one instruction per cycle, depending
on the specific instructions to be executed.}

\subsection{A Summary of the DEC model and its Variants} \label{sec:model}

The Dispersion, Extinction, and Cladogenesis (DEC) model defines the probability of observing a given biogeographical
history along a given, fixed phylogenetic tree equiped with branch lengths.
Under this model, there are three processes which govern range evolution.
The first two processes, dispersion and extinction, model the stochastic range shifts over time, by corresponding events that newly
occupy or vacate a region.
The model assumes that each region may independently transition to a different state (either from full to empty or
vise versa) with waiting times which are exponentially distributed according to a free rate parameter.
For instance, a region \( r_i \) which is full with extinction rate \( e \), the waiting time \( w \) for \( r_i \)
to transition to empty is \( w \sim \text{Exp}(e) \).
Alternatively, with a dispersion rate $d$, an empty region may transition to full with waiting time $w \sim \text{Exp}(d)$.

The third process is cladogenesis, that is, when a parent species splits into two daughter species.
During this, some regions from the parental range will be inherited by one daughter, and the remaining ones will be inherited by the
other daughter.
The particular way in which parental ranges are inherited is restricted to a limited set of scenarios.
In the original Ree \textit{et. al.}\cite{ALikelihoodFrReeR2005} paper, these scenarios are simply given as Scenario 1,
2, and 3.
These scenarios are intended to represent realistic cladogenesis events, specifically allopatry, and sympatry, while an
additional scenario was included to describe a single region cladogenesis event.
In the interest of clarity and memorability, we will use the more common names as introduced in \citet{ModelSelectionMatzke2014}.

Informally, the scenarios possible under the strict DEC model are: allopatry (alternatively, vicariance), where the
daughter ranges are disjoint; sympatry, where the daughter species share at least one range; and copy, where both
daughter ranges are identical to the parental range.
A copy event is distinct from a sympatric event as it can only occur when the parental range only comprises a single region
(a singleton).
We provide a formal definition of these cladogenesis events in Section~\ref{sec:formal-cladogenesis}.

\citet{ModelSelectionMatzke2014} extended the set of cladogenesis events by incluidng a novel ``jump" scenario that 
aims to represent ``founder-event speciation".
Here, a small population becomes isolated by colonizing a novel region.
Additionally, \citet{ModelSelectionMatzke2014} also allowed for the relative probability of the cladogenesis events to
vary between datasets.
Under this extension, sympatric events might have a larger weight, and therefore exhibit a higher likelihood, than
allopatric events.
These extensions are generally denoted by DEC+J, similar to how gamma rate categories are denoted by GTR+G4 in
standard phylogenetic models.

The parameters of the strict DEC model are the dispersion and extinction rates ($d$ and $e$), and a fixed phylogenetic tree
with fixed branch lengths. 
\decj{} adds the cladogenesis parameters $s$, $v$, $y$ and $j$, for sympatric, allopatric (vicariance), copy, and jump
events, respectively.

\subsection{A Formal Definition of Cladogenesis Events}
\label{sec:formal-cladogenesis}

We will represent a cladogenesis event as a tuple $(p, l, r)$, where $p$ is the parent range, $l$ is the left child
range, and $r$ is the right child range.
The DEC[+J] model assumes that range inheritance under speciation occurs instantaneously and is confined to a single region \todo{reword this sentence, remove instantaneously}.
Speciation is modeled as taking place in that single region.
Hence, at least one of the daughter ranges ($l$ or $r$) will be a singleton.
For the remainder of this section and without loss of generality, we assume that $l$ is the singleton range.

When a cladogenesis event occurs, the \textit{strict} DEC model assumes equal probability for all event types.
In other words, when considering if a particular cladogenesis event is more likely to be allopatric or sympatric,
the \textit{strict} DEC models these as being equally likely.
To compute the probability of a type of cladogenesis event over the set \(T = \left\{\text{Allopatry, Sympatry,
Copy}\right\}\) we compute
\[
	P(t |
	s) = \frac{C(t | p)}{\sum_{u \in T} C(u | p)},
\]
where \( C(t|p) \) is the count of possible cladogenesis events of type $t$ given the parent range, and $T$ is
the set of \textit{all} possible event types.

The cladogenesis model is extended in \decj{} by including weights and an additional cladogenesis type.
Under this extended model, we compute the probability as 
\begin{equation}
	P(t | p) = \frac{w_t C(t | p)}{\sum_{u \in T} w_u C(u | p)},
\end{equation}
where $w_t$ is the weight of cladogenesis type $t$.
Additionally, $T$ is augmented by the new type ``jump" to become \(T = \left\{\text{Allopatry, Sympatry, Copy,
Jump}\right\} \).
Informally, ``jump" events are cladogenesis events which allow one of the daughter species to disperse to an unoccupied
range.

We define the cladogenesis event type as
\begin{equation}
T(e) = T(p, l, r) := 
\begin{cases}
  \text{Copy} & \text{if }l = r = p \\ 
  \text{Sympatric} & \text{if } \CountFull{\rand{l}{r}} = 1 \text{ and } \ror{l}{r} = p \\ 
  \text{Allopatric} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } \ror{l}{r} = p   \\ 
  \text{Jump} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } r = p. \\ 
\end{cases}
\label{eq:clad-test}
\end{equation}

\subsection{Further \decj Model Extensions}

A common practice in \decj{} analyses is to include information about the ease of dispersions or jumps occurring
between two areas.
This information is typically provided via a connectivity graph, which represents the connectivity between pairs of regions by a
positive number.
These connectivities are often derived by using the geographic distance between a pair of regions.
Then, the dispersion rate and jump weight between a region pair $i$ and $j$ are modified via the provided connectivities
as
\begin{align*}
  d_{ij} &= d c_{ij}^w \\
  j_{ij} &= j c_{ij}^w,
\end{align*}
where $w$ is an inferred parameter and $c_{ij}$ is the user-supplied connectivity between $i$ and $j$.

Some other authors might distinguish between connectivity and distance matrices.
However, regardless of whether a connectivity matrix or a distance matrix is used, the main mathematical operation remains identical.
The only difference is the exponent $w$: it is positive for connectivity matrices, and negative for
distance matrices.
Given that they represent the same underlying concept, we will not further distinguish between distance and connectivity matrices.

\section{Methods}

For generating data under the DEC[+J] model, two simulation phases are required. 
Range changes occurring along a branch (dispersion and extinction) and ranges that occur as a result of speciation events (cladogensis) need to be simulated as separate processes.
For mnemonics, we will refer to these phases as the \textit{spread} and \textit{split} phases.

As already mentioned, for both phases, we implemented two completely independent data simulation methods in order to verify that the
results of these two distinct implementations are numerically identical. 
The first simulation method (for both the spread and split phases) is based on the design pattern of rejection
sampling.
In rejection sampling, samples of a distribution over a complicated sample (say $\Omega$) space are generated by
initially sampling from a distribution over a simpler sample space (say $\Omega'$). 
If a sample drawn from $\Omega'$ is not in $\Omega$, then the sample is rejected as invalid, and we draw a new sample.
This process is repeated until we draw a valid sample\footnotemark.
Rejection sampling can be modified to draw non-uniform distributions by means of rejecting valid samples
by some specified proportion in order to correct for the bias.
The precise rejection probability in this case depends on the specific distributions. 
We will specify the rejection probability in detail when we discuss each method.

\footnotetext{This process can fail to terminate with probability $\lim\limits_{n \to \infty} P(\text{Reject})^n = 0$}

Rejection methods are extremely simple, and therefore easy to implement correctly.
The unfortunate trade-off is that a substantial amount of time is spent on generating ultimately rejected samples. 
The time spent generating invalid samples is ultimately wasted.
Therefore implementing software will necessarily be, to some degree, computationally inefficient.
Rejection sampling is typically only utilized when no other method is capable of producing acceptable results.

In addition to the rejection methods, we implemented ``fast" methods of generating samples for the spread and split phases.
These ``fast" methods implement two major improvements: analytic and CPU-aware optimizations.
Here, an analytic optimization relies on a fast algorithm which will analytically generate samples from the desired distribution.
However, we need to keep in mind that the analytical correctness of an algorithm does not imply that it is \textit{numerically} correct
\cite{goldberg_what_1991, noauthor_ieee_1985}.
In addition, we implement CPU-aware optimizations which leverage the computational capabilities of modern CPUs.

We can then use these independent implementations (rejection and ``fast" method) to verify the simulation results.
By verifying that the results of the two methods are equivalent, in a rigorous statistical sense, we can
deduce that if there is a software bug, it must be present in both methods.
However, the probability of the same software bug being present in two independent and algorithmically distinct different
implementations is extremely low \cite{sklaroff1976, taneja2010}.
Therefore, if the results of the two methods produce statistically equivalent results, we can be confident that the
results are correct.

% \todo[inline, color=green!20]{The text has been edited up to here. Beyond this is a rough draft.}

\subsection{Simulating the Spread}

In the \decj{} model, the dispersion and extinction processes are modeled as a continous time markov chain (CMTC).
CMTCs are themselves a generalization of many independent Poisson processes.
We use this fact to draw samples to generate valid spread events.
A spread event $e = (p, c, w)$, where $p$ is the parent range, $c$ is the child range, and $w$ is the waiting time
between $p$ transition to $c$.
For any spread event $p$ and $c$ differ by in only one region.
More formally $\CountFull{\rxor{p}{c}} = 1$.

The problem is this: given a range \( r \) with \( |r| = n \) regions and branch length \( t \), produce a final range
which has undergone the processes of extinction and dispersion at rates of \( e \) and \( d \), respectively.
Each region experiences either extinction or dispersion independently of each other.
This observation is the basis for the rejection method of spread simulation: sample $n$ waiting times,
labeled \( w_i \), one for each region.
Specifically, the waiting times are distributed as
\begin{equation}
	\label{eq:exp-rejection} w_i \sim
	\begin{cases}
		\text{Exp}(e) & \text{if } r_i = 1 \\
    \text{Exp}(d) & \text{if } r_i = 0.
	\end{cases}
\end{equation}
Once all $n$ waiting times have been sampled, we find $ i = \text{argmin}(w_i)$ and negate the corresponding
region so that $r_i' := \rneg{r_i}$.
The resulting transition event is $(r, r', w_i)$.

We repeat the above process until the total waiting, the sum of selected $w_i$'s, time exceeds $t$, at which point the
process halts, and we yield the list of transition events, excluding the final generated event.

Keen readers might already be aware of the fact that if we have a set of independent processes as above, the entire set
can be represented via a single exponential distribution, with \(w \sim \text{Exp}(q) \) where
\begin{equation}
	\label{eq:exp-param} q = e \times \CountFull{r} + d \times \CountEmpty{r}.
\end{equation}
This second method then is to compute \( t \) using Eq.~\ref{eq:exp-param}, draw a waiting waiting time \( w \sim
\text{Exp}(t) \).
Once a time is sampled, we pick a region with weights equal to the exponential distribution parameter in
Eq.~\ref{eq:exp-rejection}.

We implement both of the above methods, but only use the second to generate samples in normal execution.
The first method, the rejection method, is used to check the results of the second method, the faster method.

\subsection{Simulating the Split}

A split will be written as an ordered triplet of ranges \( (p,l,r) \), where $p$ is the parent range and $l$ and $r$ are
the respective child ranges. 
Given $p$, we can sample a split by sampling two random numbers from a uniform distribution, and rejecting invalid
splits.
That is, we accept splits which fall into one of the categories defined in Section~\ref{sec:model}.

\begin{align}
  P(\text{Sym} | p) &= \frac{\CountFull{p} \times 1 \times 2}{(2^2n)}
\end{align}
Because we there is one way to pick a region which equals $p$, and there are $\CountFull{p}$ ways to pick a region which
is a singleton and a subset of $p$. Additionally, the left and right child can be swapped.

By similar logic, there are $\CountFull{p}$ ways to pick a region such that $\CountFull{l} = \CountFull{p} - 1$. 
Once $l$ has been picked, there is only one region which will produce a valid split event.
Therefore, 
\begin{align}
  P(\text{Allo} | p) &= \frac{\CountFull{p} \times 1 \times 2}{(2^2n)}
\end{align}
which is equal to $P(\text{Sym} | p)$.
Therefore, for distributions where $j = 0$, the probability of drawing either a sympatric or allopatric event is the
correct.
Additionally, since the probability of any particular $l$ and $r$ is equal given a split type, then all valid splits
have the same probability.

In the case when $j \neq 0$, then
\begin{align}
  P(\text{Jump} | p) &= \frac{\CountEmpty{p} \times 1 \times 2}{(2^2n)}
\end{align}
In order to implement the weighted split type introduced by Matzke in \cite{ModelSelectionMatzke2014}, we accept a
sampled split event with probability that is equal to the normalized split weight.
For example, if the split type weights are $y = 1.0, s = 1.0, v = 1.0, j=1.0$, then a jump type split will be accepted
with probability $j/(y + s + v + j)$.

To accelerate this process, we implement an optimized split simulation procedure.
First, we generate the split type according to the relative split event weights: $y, s, v,$ and $j$.
Sampling is a split in this case is divided into two cases, the singleton case where $\CountFull{p} = 1$, and the
non-singleton case.
In the singleton case, we generate a split type from $\{\text{jump}, \text{copy}\}$, weighted accordingly.
In the non-singleton case, we generate a split type from $\{\text{jump}, \text{sympatry}, \text{allopatry}\}$, also
weighted accordingly.

We compute the weight for each type as
\begin{equation}
	w_s = C(s|r) \times u_s
\end{equation}
where $C(s|r)$ is the count of feasible splits of type $s$ given a region $r$.
For example, for region \texttt{11} there exists two ways to split allopatrically: $(\texttt{10}, \texttt{01})$ and
$(\texttt{01}, \texttt{10})$
The exact formulas for various types are
\begin{align*}
  C(\text{Allo}|r) = C(\text{Sym}|r)  & = \CountFull{r} \times 2 \\ 
  C(\text{Jump}|r) & = \CountEmpty{r} \times 2 \\
  C(\text{Copy}|r) & = 2 \\
\end{align*}

Once a split type has been sampled, we generate a split according to this type by initially determining which region
will be flipped\footnotemark.
If the split type is a jump, the region is chosen from among the empty regions.
If the split type is either allopatry or sympatry, the region is chosen from the full regions.
Once the region is flipped, we randomly determine which child obtains which region, with uniform probability.

\footnotetext{If the type is copy, we can simply return the tuple $(r, r, r)$.}

\subsubsection{Simulating a Phylogenetic Tree}

Given a root range $r$, and parameters $d$, $e$ and $c$, and an age $t$, we can sample a phylogenetic tree with
Algorithm~\ref{alg:tree-sample}.
In short, a tree is sampled by starting with a root range $r$ and a tree age $t$. 
The range $r$ is then evolved under the \decj{} process.
Waiting times until an event, either a spread or split event, are sampled, and then the corresponding event is generated
with $r$ as the parent range.
When the waiting time for a branch exceeds $t$, the process halts.
We recursively iterate over all branches utill all extant tips have a distance to the root of $t$.

\begin{algorithm}
  \SetKwFunction{KwFn}{SampleTree}
  \SetKwProg{Fn}{Function}{:}{}

  \Fn{\KwFn{$p,t,e,d,c$}}{
  $u := d\CountFull{r} + e\CountEmpty{r}$\\
  $w \sim \mathrm{Exp}(u + c)$\\
  \If {$w > t$} { 
    \Return
  }
  $t := t - w$\\
  $l \sim \mathrm{Bernoulli}(\frac{u}{u+c})$\\
  \If {$l == 1$} {
    $(p, p', w) \sim \mathrm{Spread}(e,d)$\\
    \If{ $p' = \emptyset$} { \Return }
    \KwFn{$p',t,e,d,c$}\\
  }
  \Else {
    $(p, l, r) \sim \mathrm{Split}(c)$\\
    \KwFn{$l, t, e, d, c$}\\
    \KwFn{$r, t, e, d, c$}\\
    \Return
  }
}
\caption{Sample a tree under \decj, given a deadline $t$. \label{alg:tree-sample}}
\end{algorithm}

\subsection{Testing}

Our primary strategy for testing for correctness is to test the two processes, split and spread, independently with
statistical rejection testing with extremely large sample sizes to minimize the chance and degree of error.
To this end, we implemented our tests in C++, using the core functions of \bigrig{} as a library.
Writing the tests in a performant language like C++ allows us to compute large sample sizes in a reasonable amount of
time.

\subsubsection{Spread}

For ensuring correct results and implementations, we conduct two types of tests.
The first test computes the expected value of the waiting time given the model parameters analytically, and performs a
t-test against that expected value.
We compute 1,886,084,219 samples of the spread distribution, which allows us to be 99.999\% confident that the error is
less than 0.0001.
We perform this test for 7 different initial ranges with, each with a size of 4 areas, and 16 different parameter sets,
for a total of 116 separate tests.

The second test is a regression test against the rejection method.
We again run both methods for 1,886,084,219 iterations, which again ensures that the error is less than 0.0001 with
99.999\% confidence.
Furthermore, we also perform this test for 5 different initial ranges, each with a size of 4 areas, and 16 different
parameter sets, for a total of 80 separate tests.

Finally, we conducted a $\chi^2$ test to ensure that regions are being picked at the expected rate.
For this test, we enabled either dispersion or extinction, including both, and computed the expected number of
substitutions for each area given the initial range.
With this initial region, we sampled 1000 spread events, and the utilized a $\chi^2$ test to ensure the realized
distribution matches the expected distribution.
We reject the hypothesis that the two distributions are equivalent with $\alpha=0.0001$, I.E. we reject with 99\%
confidence.

\subsubsection{Split}

We compute a sample of 2,019,696,124 split events, and compare this sample against the expected distribution of
cladogenesis event types using a G test.
This gives a probability of either type I or type II error occurring at 0.00001, with a maximum deviation from the
expected value of 0.0001.
Please see the supplemental material for full derivation of these values.
We perform a test for each of 6 initial ranges and 5 cladogenesis parameters for a total of 30 tests.

Likewise, we also compare the results of the rejection method with the analytical method.
Due to the expensive nature of rejection sampling, we limit our sample size to 201,970 split events from each of the
methods.
We then use the same G test to compare the results.
We perform this test for 6 initial ranges and 5 sets of cladogenesis parameters for a total of 30 tests.

Finally, we conducted a $\chi^2$ test to ensure that regions are distributed as expected between the child regions.
For event types Sympatry, Allopatry, and Jump, we sample a split event of the specified type and determine the singleton
region.
We then ensure that realized counts match the expected counts with a $\chi^2$ test with rejection threshold of $\alpha =
0.0001$.

\subsection{Benchmarks}

We also sought to evaluate the performance of \bigrig{}.
We generated 10 random rooted ultrametric trees with $2^i, 3 \leq i \leq 16$ each, for a total of 140 trees.
These trees were generated via a Yule process, and then the branch lengths scaled so that the tree hight was equal to
1.0.
For each tree generated, we generated 10 random ranges of size $3, 7, 15, 31$, along with the CMTC model parameters $e,d
\sim \mathrm{Uniform}(0,1)$ for a total of 50 datasets for each tree, 10 for each range size, for a total of 7000
trials.
Cladogenesis parameters were set to $s = v = y = 1$ and $j=0$ for all trials.
We implemented this pipeline in Snakemake \todo{cite} and we measured execution times with the Linux tool
\texttt{perf} \todo{cite}.
The execution times of \bigrig{} tend to be very short, so to measure with sufficient resolution we found it was
utilize \texttt{perf} which uses kernel and hardware level performance counters to measure runtime.

\todo[inline]{Should I also add tree sampling benchmarks?}

\section{Results}

For all tests we conducted, \bigrig{} passed.
As each statistical test can be considered independent, we can estimate the probability of an implementation error in
\bigrig{} going undetected as the product of a type I error for each test.
To be explicit, the probability of an undetected error is nearly 0, and we can be nearly certain that no such bug exists
in \bigrig{}.

In general, \bigrig{} is extremely fast, as can be seen in the Fig.~\ref{fig:runtime-taxa}. The mean time for execution
varies from 0.0042 seconds with 3 ranges to 0.0066 seconds with 63 ranges. The minimum time over all trials was 0.000004
seconds, and the max time was 0.081 seconds.

\todo[inline]{text about the impact of the jump parameter on execution time}

\begin{figure}
    \centering
    \includesvg[width=\linewidth, pretex=\relscale{0.5}]{figs/bigrig.times.linreg.svg}
    \caption{Plot of smoothed generalized additive model (GAM) fit to runtime data for bigrig execution.
      Plot was generated with \texttt{ggplot2}'s \texttt{geom\_smooth()} function.
      For each taxa quantity, 10 random rooted trees were generated via a Yule process with branch lengths scaled to a total
      tree hight of 1.0.
      For each tree, 10 sets of CMTC model parameters were randomly sampled such that $e, d \sim \mathrm{Uniform}(0,1)$.
      Cladogenesis parameters were set to $s = v = y = 1.0$ and $j = 0.0$.
      Shaded area indicates the 0.95 confidence band as computed by the \texttt{predict.Gam} function in R.
    }\label{fig:runtime-taxa}
\end{figure}

\section{Conclusion}

We have presented \bigrig{}, a reliable, fast, and easy to use tool to generate simulated historical biogeography
datasets.
We have shown that \bigrig{} has a negligible chance of having a software implementation error.
We have also show that it is extremely fast, capable of generating a dataset in under a second which is far beyond the
ability of any current inference software to infer.

In the future, we wish extend the features of \bigrig{} to include dataset generation for the family of State Specific
Evolution (SSE) models (BiSSE, GeoSSE, ClaSSE) \todo{cite}.
The primary purpose of these extensions is to test the robustness of \decj{} models to model violations, in order to
asses the added value of these models.
This analysis is important for future tool development, as the more advanced SSE member models cannot be inferred using
the typical CMTC computational framework use for \decj{}.
However, at this time it seems like these models will exhibit similar scaling as \decj{} as all models here share the
exponential scaling of their state space.

Investigating exactly when \decj{} fails to give adequate results will help both users and tool developers to
efficiently utilize their scarce resources.
In the case of users, it will help conserve compute time, and in the case of tool developers it will help conserve
developer time.

\section{Acknowledgements}

Many thanks to Alexander I. Jordan from the Computational Statistics group at the Heidelberg Institute for Theoretical
Studies for his assistance on deriving and implementing the statistical tests used to verify the results of \bigrig{}.

\bibliography{references}

\end{document}
