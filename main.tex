%! TeX program = lualatex

\documentclass[a4paper]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{geometry}[margins=1in]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{relsize}
\usepackage{todonotes}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{lineno}
\usepackage{microtype}
\usepackage{hyperref}

\bibliographystyle{sysbio}


\newcommand{\CountFull}[1]{|#1|_\text{full}}
\newcommand{\CountEmpty}[1]{|#1|_\text{empty}}
\newcommand{\bigrig}{\texttt{bigrig}}
\newcommand{\decj}{DEC[+J]}
\newcommand{\rand}[2]{#1 \land #2}
\newcommand{\ror}[2]{#1 \lor #2}
\newcommand{\rneg}[1]{\neg #1}
\newcommand{\rxor}[2]{#1 \oplus #2}
\newcommand{\rLshift}[2]{#1 \ll #2}
\newcommand{\rRshift}[2]{#1 \gg #2}

\title{\bigrig{}: A range simulator for the DEC[+J] model}
\author{Ben Bettisworth and Alexis Stamatakis}

\todostyle{ingreen}{color = green!40, inline}

% \linenumbers

\begin{document}

\maketitle

\abstract{
Quality software tools for science need to be rigorously tested and verified. 
However, there is a major challenge to testing software used for phylogenetics and similar analysis.
As a field, we have a shortage of ground truth data which can be used for validation, resulting in a reliance on
simulated data.
This reliance on simulated data results in a second challenge: verifying the correctness of the tool which
generates the simulated data.
In historical biogeography, software which simulated data has exclusively been implemented on an ad-hoc basis to verify
specific tools.
Here, we introduce \bigrig{}, a simulator for the \decj{} model of range evolution.
We show that \bigrig{} is correct via rigorous statistical testing and validation using two completely independent
simulation algorithms
We show thawt results between these independent implementations do not deviate by more than 0.0001 with 99.999\%
confidence.
We also show that \bigrig{} is extremely fast, capable of generating data for trees with tens of thousands of tips in
under a second.
}

\section{Introduction}

Verifying the correctness of phylogenetics software is a perpetual challenge for tool developers
\citep{darribaStateSoftwareEvolutionary2018, mendesHowValidateBayesian2025, carver_software_2007,
bettisworth_lagrange-ng_2023}. 
The true phylogenetic tree\textemdash the series of speciation events originating from a common ancestor which explain an
evolutionary history\textemdash is almost always unobservable. 
It is generally impossible to verify the results generated by software tools with a true, empirical reference phylogeny
(but see \cite{hillis_experimental_1992} for one of the few notable exceptions.) 
As it currently stands, datasets for which the truth is known are exceptionally rare.
Therefore, researchers cannot rely solely on these occasional known phylogenies to verify tool correctness.

A common goal in historical biogeography is to infer the ancestral ranges of species along a known phylogenetic tree
\citep{varela_phylogeny_2019, baker_global_2013, vicente_and_2017}.
However, similar to the true phylogenetic tree, these ancestral ranges generally can not be directly observed.
Some ancestral ranges can be estimated from the fossil record \citep{mclachlan_reconstructing_2004}, but there can be
multiple complications which limit our knowledge about the true ancestral ranges.
These complications include, but are not limited to: gaps in the fossil record; the study organism consists only of soft
tissue which does not fossilize; or a geographical distribution which is unsuited for fossilization
\citep{kidwell_quality_2002}.
Overall, there does not exist a reliable source of known ancestral ranges and associated phylogenetic trees. 

Nonetheless, software verification remains vital, as without it is challenging to discriminate between a surprising
result and an erroneous result which is the result of a software error.
Therefore, one must turn to simulations in order to generate the necessary data to verify the correctness of the
respective software tools \citep{mendesHowValidateBayesian2025, ly-trong_alisim_2022, fletcher_indelible_2009}.
Generally, artificial data are produced by assuming some statistical model of evolution, and subsequently generating
datasets with a given set of model parameters.
Data generated with this approach are unrealistic as they fail to capture the complexities of real biological systems
\citep{trost_simulations_2024}.
Despite this, the use of simulated data is still ubiquitous for software verification as currently no viable alternative
exists.

However, software simulators are subject to the same concerns as inference software, specifically the presence of
software errors can subtly corrupt the results.
For example, these simulations might utilize the same code base for the inference of ancestral ranges and corresponding
model parameters. 
In such cases, it becomes more likely that a software error in the model inference will also corrupt simulation results.
Therefore, separating simulation code from inference code reduces the chances of integrated bugs in both the simulation
and the inference.
Additionally, it is critical that tools which implement inferences under a specific model are assessed using a common
(simulation) standard.
By using a common standard to evaluate tools one can ensure that improved performance (either computational performance
or inference accuracy) is not merely a consequence of the evaluation process.

A common simulator also decreases the amount of work required to implement a new inference tool, as a fast and reliable
method of generating testing data can be utilized by developers in the context of iterative development cycles.
This will have the (eventual) result of increasing the number of tools available, and will increase the overall quality
of tooling.

To date, biogeographical data simulations under the DEC[+J] model have often been implemented in an ad-hoc manner, with
individual tools implementing their own simulation framework \citep{matzke_statistical_2022,
bettisworth_lagrange-ng_2023}.
This is done even though tools which can generate simulated data do exist (E.g. RevBayes \citep{hohna_revbayes_2016}).
Indeed, RevBayes is capable of generating data under nearly any statistical model, but this comes at the cost of
usability, as the user must specify the model themselves. 
This model specification task is non-trivial, and without expertise there is a high risk of inadvertently specifying an
incorrect model.
Therefore, instead of implementing ad hoc simulations for each software project, we can reduce the likelihood of errors
by developing an independent stand-alone simulation tool.

In this paper we present \bigrig{}, a simulator for the \decj{}  model which complies with all of the aforementioned
criteria for simulators.
We explain the \decj{} model in detail in Section~\ref{sec:model}.
Given a set of \decj{} parameters and a phylogenetic tree, \bigrig{} will generate a range dataset for the taxa at the
tips of the tree.
In addition, ranges for inner nodes, cladogenesis events, and state transitions along branches will be generated.
After generation, \bigrig{} will log the results in a variety of practical file formats (YAML, JSON, or CSV), as well as
output the results in phylip format, and an annotated tree in Newick format.

Furthermore, we show that \bigrig{} is both highly reliable and extremely computationally efficient in regards to
runtime and memory usage. We show that \bigrig{} is reliable via two distinct approaches.
First, we derive expected distributions for the results of fundamental model events, and perform statistical
tests to ensure that the results are within 0.0001 with 99.999\% of type 1 or type 2 error.
Second, we produce an independent implementation which uses a slower, yet also simpler-to-implement method.
Subsequently, we conduct statistical tests to verify that the results of these two independent implementations agree.
Finally, we show that \bigrig{} is computationally efficient.
We demonstrate that \bigrig{} can generate datasets for trees with tens of thousands of tips and 63 regions in under 0.5
seconds on a mid-range laptop.

% \todo[ingreen]{Simpify this section. Specifically, I need to explain why other simulators aren't typically used, and
% the difficulty of using RevBayes.}

\section{Background}

In the context of historical biogeography, an individual geographic sector where a taxon may be present is refered to as
a \textit{region}.
A region can either be occupied, in which case we say it is full, or it can be unoccupied, in which case we say it is
empty.
A \textit{range} describes whether each region is full or empty for a specific taxon for every region in a set.
We denote the total number of regions for a range \( r \) as \( |r| \).
We write the number of full regions a range has as \( \CountFull{r} \), and the number of empty regions as \(
\CountEmpty{r} \).
We denote the value of the $i$th region of a range as $r_i$.
In practice and for simplicity's sake, we represent ranges as binary strings, with $1$ indicating a full region, and $0$
denoting an empty region.

For the regions $r$ and $s$ we denote the \textit{bitwise and} operation as $\rand{r}{s}$, the \textit{bitwise or} as
$\ror{r}{s}$, the \textit{bitwise exclusive or} as $\rxor{r}{s}$, and the \textit{bitwise negation} as $\rneg{r}$.
Here, \textit{bitwise} indicates that the operation is executed independently on each character in the string (that is,
each region). 
For example the \textit{bitwise and} is defined as detail $r_i \land s_i = t_i$ where $0 \leq i \leq |r|$, $\texttt{1}
\land \texttt{1} = \texttt{1}$ and is equal to \texttt{0} otherwise.

These operations are of a particular interest as they can be efficiently computed via fundamental CPU instructions.
Consequently, computing the results of these operations often requires less than a single CPU cycle\footnotemark
\citep{Abel19a}.
As such, computation of these operations (and functions which are composed of these operations) is extremely efficient.

\footnotetext{This is due to the capabilities of modern CPUs to potentially execute more than one instruction per cycle,
depending on the which instructions are executed.}

\subsection{An Overview of \decj{}} \label{sec:model}

The Dispersion, Extinction, and Cladogenesis (DEC) model defines the probability of observing a given biogeographical
history along a fixed phylogenetic tree with branch lengths.
Under this model, there are three processes which govern range evolution.
The first two, dispersion and extinction, model the stochastic range shifts over time, by modeling state changes
that change a region to full or empty.
The model assumes that each region may independently transition to a different state with waiting times which are
exponentially distributed according to one of two rate parameters.
For instance, with a full region \( r_i \) and extinction rate \( e \), the waiting time \( w \) for \( r_i \)
to transition to empty is \( w \sim \text{Exp}(e) \).
Alternatively, with a dispersion rate $d$, an empty region may transition to full with waiting time $w \sim
\text{Exp}(d)$.

Cladogenesis, the third process in the DEC model, is when a parent species splits into two daughter species.
During this event, full regions from the parent range are divided between the two daughters.
The particular way in which parental ranges are inherited is restricted to a limited set of scenarios.
In the original \cite{ALikelihoodFrReeR2005} paper, these scenarios are simply given as Scenario 1,
2, and 3.
The first two scenarios are intended to represent the pair of realistic cladogenic events, allopatry and sympatry.
Scenario 3 covers the case when the parent range consists of a single full region, and so the two daughter species must
inherit the same range.
In the interest of clarity and memorability, we will use the names allopatry, sympatry, and copy to describe these
scenarios.

Informally, the cladogenesis scenarios possible under the strict DEC model are: daughter ranges are disjoint
(allopatry, alternatively vicariance); daughter species share at least one range (sympatry); daughter ranges
are identical to the parent's range, and all regions are singletons (copy).
We provide a formal definition of these cladogenesis events in Section~\ref{sec:formal-cladogenesis}.

\cite{ModelSelectionMatzke2014} extended the set of cladogenesis events by incluidng a novel ``jump" scenario that 
aims to represent a ``founder-event speciation" event.
Here, a small population becomes isolated by colonizing a novel region.
Additionally, \cite{ModelSelectionMatzke2014} also allowed for the relative probability of the cladogenesis events to
be inferred parameters, such that they may vary between datasets.
Under this extension, sympatric events might have a larger weight, and therefore exhibit a higher likelihood when
observed, than allopatric events.
These extensions are generally denoted by DEC+J, similar to how gamma rate categories are denoted by GTR+G4 in
standard phylogenetic models.

The parameters of the strict DEC model are the dispersion and extinction rates ($d$ and $e$), and a fixed phylogenetic tree
with fixed branch lengths. 
\decj{} adds the cladogenesis parameters $s$, $v$, $y$ and $j$, for sympatric, allopatric (vicariance), copy, and jump
events, respectively.

\subsection{A Formal Definition of Cladogenesis Events}
\label{sec:formal-cladogenesis}

We will represent a cladogenesis event as the tuple $(p, l, r)$, where $p$ is the parent range, and $l$ and $r$ are the
left and right child ranges, respectively.
The DEC[+J] model assumes that specitation (and therefore range inheritance) occurs at an instant in time and is
restricted to a single region.
This implies that at least one of the daughter ranges ($l$ or $r$) will be a singleton.
For the remainder of this section and without loss of generality, we assume that $l$ is the singleton range.

When a cladogenesis event occurs, the \textit{strict} DEC model assumes equal probability for all event types.
In other words, when considering if a particular cladogenesis event is more likely to be allopatric or sympatric,
the \textit{strict} DEC models these as being equally likely.
To compute the probability of a type of cladogenesis event over the set \(T = \left\{\text{Allopatry, Sympatry,
Copy}\right\}\) we compute
\[
	P(t |
	s) = \frac{C(t | p)}{\sum_{u \in T} C(u | p)},
\]
where \( C(t|p) \) is the count of possible cladogenesis events of type $t$ given $p$, and $T$ is the set of
\textit{all} possible event types.

This cladogenesis model is extended in \decj{} by including weights and an additional cladogenesis type.
Under this extended model, we compute the probability as 
\begin{equation*}
	P(t | p) = \frac{w_t C(t | p)}{\sum_{u \in T} w_u C(u | p)},
\end{equation*}
where $w_t$ is the weight of cladogenesis type $t$.
Additionally, $T$ is augmented by the new type ``jump" to become \(T = \left\{\text{Allopatry, Sympatry, Copy,
Jump}\right\} \).
Informally, ``jump" events are cladogenesis events which allow one of the daughter species to disperse to an unoccupied
range.

We define the cladogenesis event type as
\begin{equation*}
T(e) = T(p, l, r) := 
\begin{cases}
  \text{Copy} & \text{if }l = r = p \\ 
  \text{Sympatric} & \text{if } \CountFull{\rand{l}{r}} = 1 \text{ and } \ror{l}{r} = p \\ 
  \text{Allopatric} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } \ror{l}{r} = p   \\ 
  \text{Jump} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } r = p. \\ 
\end{cases}
\label{eq:clad-test}
\end{equation*}

%\subsection{Further \decj Model Extensions}
%
%A common practice in \decj{} analyses is to include information about the ease of dispersions or jumps occurring
%between two regions.
%This information is typically provided via a connectivity graph, which represents the connectivity between pairs of
%regions by a positive number.
%These connectivities are often derived by using the geographic distance between a pair of regions.
%Then, the dispersion rate and jump weight between a region pair $i$ and $j$ are modified via the provided connectivities
%as
%\begin{align*}
%  d_{ij} &= d c_{ij}^w \\
%  j_{ij} &= j c_{ij}^w,
%\end{align*}
%where $w$ is an inferred parameter and $c_{ij}$ is the user-supplied connectivity between $i$ and $j$.
%
%Some other authors might distinguish between connectivity and distance matrices.
%However, regardless of whether a connectivity matrix or a distance matrix is used, the primary mathematical operations
%are identical.
%The only difference is the exponent $w$: it is positive for connectivity matrices, and negative for distance matrices.
%Given that they represent the same underlying concept, we will use connectivity matrix to refer to either matrix.

\section{Methods}

For generating data under the DEC[+J] model, two simulation phases are required. 
Range changes occurring along a branch (dispersion and extinction) and ranges that occur as a result of speciation
events (cladogensis) need to be simulated as separate processes.
For mnemonics, we will refer to these phases as the \textit{spread} and \textit{split} phases.

As already mentioned, for both phases, we implemented two completely independent data simulation methods in order to
verify that the results of these two distinct implementations are numerically identical. 
The first simulation method (for both the spread and split phases) is based on the design pattern of rejection
sampling.
In rejection sampling, samples of a distribution over a complicated sample (say $\Omega$) space are generated by
initially sampling from a distribution over a simpler sample space (say $\Omega'$). 
If a sample drawn from $\Omega'$ is not in $\Omega$, then the sample is rejected as invalid, and we draw a new sample.
This process is repeated until we draw a valid sample\footnotemark.
Rejection sampling can be modified to draw non-uniform distributions by means of rejecting valid samples
by some specified proportion in order to correct for the bias.
The precise rejection probability in this case depends on the specific distributions.
We will specify the rejection probability in detail when we discuss each method.

\footnotetext{This process can fail to terminate with probability $\lim\limits_{n \to \infty} P(\text{Reject})^n = 0$}

Rejection methods are extremely simple, and therefore easy to implement correctly.
The unfortunate trade-off is that a substantial amount of time is spent on generating ultimately rejected samples. 
The time spent generating invalid samples is ultimately wasted.
Therefore implementing software will necessarily be, to some degree, computationally inefficient.
Rejection sampling is typically only utilized when no other method is capable of producing acceptable results.

In addition to the rejection methods, we implemented ``fast" methods of generating samples for the spread and split phases.
These ``fast" methods implement two major improvements: analytic and CPU-aware optimizations.
Here, an analytic optimization relies on a fast algorithm which will analytically generate samples from the desired
distribution.
However, we need to keep in mind that the analytical correctness of an algorithm does not imply that it is
\textit{numerically} correct \citep{goldberg_what_1991, noauthor_ieee_1985}.
In addition, we implement CPU-aware optimizations which leverage the computational capabilities of modern CPUs.

We can then use these independent implementations (rejection and ``fast" method) to verify the simulation results.
By verifying that the results of the two methods are equivalent, in a rigorous statistical sense, we can
deduce that if there is a software bug, it must be present in both methods.
However, the probability of the same software bug being present in two independent and algorithmically distinct different
implementations is extremely low \citep{sklaroff1976, taneja2010}.
Therefore, if the results of the two methods produce statistically equivalent results, we can be confident that the
results are correct.

\subsection{Simulating the Spread}

In the \decj{} model, the dispersion and extinction processes are modeled as a continous time markov chain (CMTC).
CMTCs are themselves a generalization of many independent Poisson processes.
We use this fact to draw samples to generate valid spread events.
A spread event $e$ is defined as the tuple $(p, c, w)$, where $p$ is the parent range, $c$ is the child range, and $w$
is the waiting time between $p$ transition to $c$.
For any spread event $p$ and $c$ differ by in only one region.
More formally $\CountFull{\rxor{p}{c}} = 1$.

The problem is this: given a range \( r \) with \( |r| = n \) regions and branch length \( t \), produce a final range
which has undergone the processes of extinction and dispersion at rates of \( e \) and \( d \), respectively.
Each region experiences either extinction or dispersion independently of each other.
This observation is the basis for the rejection method of spread simulation: sample $n$ waiting times,
labeled \( w_i \), one for each region.
Specifically, the waiting times are distributed as
\begin{equation*}
	\label{eq:exp-rejection} w_i \sim
	\begin{cases}
		\text{Exp}(e) & \text{if } r_i = 1 \\
    \text{Exp}(d) & \text{if } r_i = 0.
	\end{cases}
\end{equation*}
Once all $n$ waiting times have been sampled, we find $ i = \text{argmin}(w_i)$ and negate the corresponding
region so that $r_i' := \rneg{r_i}$.
The resulting transition event is $(r, r', w_i)$.

We repeat the above process until the total waiting, the sum of selected $w_i$'s, time exceeds $t$, at which point the
process halts, and we yield the list of transition events, excluding the final generated event.

Keen readers might already be aware of the fact that if we have a set of independent processes as above, the entire set
can be represented via a single exponential distribution, with \(w \sim \text{Exp}(q) \) where
\begin{equation*}
	\label{eq:exp-param} q = e \times \CountFull{r} + d \times \CountEmpty{r}.
\end{equation*}
This second method then is to compute \( t \) using Eq.~\ref{eq:exp-param}, draw a waiting waiting time \( w \sim
\text{Exp}(t) \).
Once a time is sampled, we pick a region with weights equal to the exponential distribution parameter in
Eq.~\ref{eq:exp-rejection}.

We implement both of the above methods, but only use the second to generate samples in by default.
The first method, the rejection method, is used to check the results of the second method, the faster method.

\subsection{Simulating the Split}

A split will be written as an ordered triplet of ranges \( (p,l,r) \), where $p$ is the parent range and $l$ and $r$ are
the respective child ranges. 
Given $p$, we can sample a split by sampling two random numbers from a uniform distribution, and rejecting invalid
splits.
That is, we accept splits which are sympatric with the probability

\begin{align*}
  P(\text{Sym} | p) &= \frac{\CountFull{p} \times 1 \times 2}{(2^2n)}
\end{align*}
as there is one way to pick a region which equals $p$, and there are $\CountFull{p}$ ways to pick a region which is
both a singleton and a subset of $p$. 
Furthermore, the left and right child can be swapped.

By similar logic, there are $\CountFull{p}$ ways to pick a region such that $\CountFull{l} = \CountFull{p} - 1$. 
Once $l$ has been picked, there is only one region which will produce a valid split event.
Therefore, 
\begin{align*}
  P(\text{Allo} | p) &= \frac{\CountFull{p} \times 1 \times 2}{(2^2n)}
\end{align*}
which is equal to $P(\text{Sym} | p)$.
Therefore, for distributions where $j = 0$ and $s = v$, the probability of drawing either a sympatric or allopatric
event is the same.
Additionally, since the probability of any particular $l$ and $r$ is equal given a split type, then all valid splits
have the same probability.

In the case when $j \neq 0$, then
\begin{align}
  P(\text{Jump} | p) &= \frac{\CountEmpty{p} \times 1 \times 2}{(2^2n)}
\end{align}
In order to implement the weighted split type introduced by Matzke in \cite{ModelSelectionMatzke2014}, we accept a
sampled split event with probability that is equal to the normalized split weight.
For example, if the split type weights are $y = s = v = j = 1.0$, then a jump type split will be accepted with
probability $j/(y + s + v + j)$.

To accelerate this process, we implement an optimized split simulation procedure.
First, we generate the split type according to the relative split event weights ($y, s, v,$ and $j$).
Sampling a split is divided into two cases, the singleton case where $\CountFull{p} = 1$, and the non-singleton case
where $\CountFull{p} > 1$.
In the singleton case, we generate a split type from $\{\text{jump}, \text{copy}\}$, weighted accordingly.
In the non-singleton case, we generate a split type from $\{\text{jump}, \text{sympatry}, \text{allopatry}\}$, also
weighted accordingly.

We compute the weight for each type as
\begin{equation*}
	w_s = C(s|r) \times u_s
\end{equation*}
where $C(s|r)$ is the count of feasible splits of type $s$ given a region $r$.
For example, for region \texttt{11} there exists two ways to split allopatrically: $(\texttt{10}, \texttt{01})$ and
$(\texttt{01}, \texttt{10})$
The exact formulas for various types are
\begin{align*}
  C(\text{Allo}|r) = C(\text{Sym}|r)  & = \CountFull{r} \times 2 \\ 
  C(\text{Jump}|r) & = \CountEmpty{r} \times 2 \\
  C(\text{Copy}|r) & = 2 \\
\end{align*}

Once a split type has been sampled, we generate a the daughter ranges according to the type by randomly picking which
region will be flipped.
If the split type is a jump, the region is chosen from among the empty regions.
If the split type is either allopatry or sympatry, the region is chosen from the full regions.
Once the region is flipped and two regions are produced, we randomly determine which child obtains the singleton region,
with probability $0.5$.

\subsubsection{Simulating a Phylogenetic Tree}

Given a root range $r$, and parameters $d$, $e$ and $c$, and an age $t$, we can sample a phylogenetic tree with
Algorithm~\ref{alg:tree-sample}.
In short, a tree is sampled by starting with a root range $r$ and a tree age $t$. 
The range $r$ is then evolved under the \decj{} process.
Waiting times until an event, either a spread or split event, are sampled, and then the corresponding event is generated
with $r$ as the parent range.
When the waiting time for a branch exceeds $t$, the process halts.
We recursively iterate over all branches until the path from each extant tip to the root has a length of $t$.

\begin{algorithm}
  \SetKwFunction{KwFn}{SampleTree}
  \SetKwProg{Fn}{Function}{:}{}

  \Fn{\KwFn{$p,t,e,d,c$}}{
  $u := d\CountFull{r} + e\CountEmpty{r}$\\
  $w \sim \mathrm{Exp}(u + c)$\\
  \If {$w > t$} { 
    \Return
  }
  $t := t - w$\\
  $l \sim \mathrm{Bernoulli}(\frac{u}{u+c})$\\
  \If {$l == 1$} {
    $(p, p', w) \sim \mathrm{Spread}(e,d)$\\
    \If{ $p' = \emptyset$} { \Return }
    \KwFn{$p',t,e,d,c$}\\
  }
  \Else {
    $(p, l, r) \sim \mathrm{Split}(c)$\\
    \KwFn{$l, t, e, d, c$}\\
    \KwFn{$r, t, e, d, c$}\\
    \Return
  }
}
\caption{Sample a tree under \decj, given a deadline $t$. \label{alg:tree-sample}}
\end{algorithm}

\subsection{Testing}

Our primary strategy for testing for correctness is to test the two processes, split and spread, independently with
statistical rejection testing with extremely large sample sizes to minimize the chance and degree of error.
To this end, we implemented our tests in C++, using the core functions of \bigrig{} as a library.
Writing the tests in a performant language like C++ allows us to compute large sample sizes in a reasonable amount of
time.

\subsubsection{Spread}

For ensuring correct results and implementations, we conduct two types of tests.
The first test computes the expected value of the waiting time given the model parameters analytically, and performs a
t-test against that expected value.
We compute 1,886,084,219 samples of the spread distribution, which allows us to be 99.999\% confident that the error is
less than 0.0001.
We perform this test for 7 different initial ranges with, each with a size of 4 regions, and 16 different parameter sets,
for a total of 116 separate tests.

The second test is a regression test against the rejection method.
We again run both methods for 1,886,084,219 iterations, which again ensures that the error is less than 0.0001 with
99.999\% confidence.
Furthermore, we also perform this test for 5 different initial ranges, each with
a size of 4 regions, and 16 different parameter sets, for a total of 80 separate
tests.
We perform fewer tests due to the increased computational cost of the rejection method.

Finally, we conducted a $\chi^2$ test to ensure that regions are being picked at the expected rate.
For this test, we enabled either dispersion or extinction, including both, and computed the expected number of
substitutions for each region given the initial range.
With this initial region, we sampled 1000 spread events, and the utilized a $\chi^2$ test to ensure the realized
distribution matches the expected distribution.
We reject the hypothesis that the two distributions are equivalent with $\alpha=0.0001$, I.E. we reject with 99.99\%
confidence.

\subsubsection{Split}

We compute a sample of 2,019,696,124 split events, and compare this sample against the expected distribution of
cladogenesis event types using a G test.
This gives a probability of either type I or type II error occurring at 0.00001, with a maximum deviation from the
expected value of 0.0001.
Please see the supplemental material for full derivation of these values.
We perform a test for each of 6 initial ranges and 5 cladogenesis parameters for a total of 30 tests.

Likewise, we also compare the results of the rejection method with the analytical method.
Due to the expensive nature of rejection sampling, we limit our sample size to 201,970 split events from each of the
methods.
We then use the same G test to compare the results.
We perform this test for 6 initial ranges and 5 sets of cladogenesis parameters for a total of 30 tests.

Finally, we conducted a $\chi^2$ test to ensure that regions are distributed as expected between the child regions.
For event types Sympatry, Allopatry, and Jump, we sample a split event of the specified type and determine the singleton
region.
We then ensure that realized counts match the expected counts with a $\chi^2$ test with rejection threshold of $\alpha =
0.0001$.

\subsection{Benchmarks}

We also sought to evaluate the performance of \bigrig{}.
In our initial investigations, we found that the time to generate data was dependent on several model parameters,
in particular the tree height and the cladogenesis parameters.
Therefore, we sought to assess both the typical performance and the extreme performance of \bigrig{}.
We generated 10 random rooted ultrametric trees with $2^i, 3 \leq i \leq 16$ each, for a total of 140 trees.
These trees were generated via a Yule process, with the branch lengths scaled so that the tree height was equal to 1.0.
For each tree generated, we generated 10 random ranges of size $3, 7, 15, 31$, along with the CMTC model parameters $e,d
\sim \mathrm{Uniform}(0,1)$ and cladogenesis parameters from either without jumps ($s = v = y = 1, j = 0$) or with jumps
($s = c = y = j = 1$).
The first cladogenesis parameter set is the fixed parameters used in the original DEC, whereas the second parameter set
simply "enables" jumps to occur, I.E. converts the model to \decj{}.
This yields 10 datasets for each tree, 20 datasets for each range size, for a total of 14,000 datasets.
We implemented this pipeline in Snakemake\citep{molder2021} and we measured execution times with the Linux tool
\texttt{perf}.
The execution times of \bigrig{} tend to be very short, so to measure with sufficient resolution we found it was
required to utilize \texttt{perf} which uses kernel and hardware level performance counters to measure runtime.

In addition to the performance benchmarks, we also benchmarked the software quality using Softwipe v0.2 \citep{zapletal2021}.
Softwipe assess the code quality via a series of hureistics, and then produces a score between 0 and 10, with 0 being
low quality, and 10 being high quality.

\section{Results}

For all tests we conducted, \bigrig{} passed.
That is, all methods produced results which were equivalent between the rejection and analytic methods, and each
analytical methods produdo we have a list of softwipe ced the expected distribution.
As each statistical test can be considered independent, we can estimate the probability of an implementation error in
\bigrig{} going undetected as the product of a type I error for each test.
To be explicit, the probability of an undetected error is nearly 0, and we can be nearly certain that no such bug exists
in \bigrig{}.

In general, \bigrig{} is extremely fast, as can be seen in the Fig.~\ref{fig:runtime-taxa}. 
The mean time for execution varies from 0.030 seconds ($\mathit{SD} = 0.002$) with 3 regions and 65536 taxa to 0.046
seconds ($\mathit{SD} = 0.004$) with 63 regions and 65536 taxa.
The minimum time over all trials was 0.000004 (3 regions, 8 taxa) seconds, and the max time was 0.081 seconds (63
regions, 65536 taxa).
The overall execution times are shown in Fig.~\ref{fig:runtime-taxa}.

The jump parameter had a noticeable, but small, impact on the overall runtime of \bigrig{}.
The mean time to generate data with 63 regions and 65536 taxa when $j = 1$ was 0.048 ($\mathit{SD} = 0.002$) versus 
0.044 ($\mathit{SD} = 0.005$) with $j = 0$.
This can be seen as well in Fig.~\ref{fig:runtime-taxa}, as the increase in execution time from added regions or added
taxa is much larger than the standard deviation (the grey shaded region).
Additionally, the difference between jumps and no jumps is statistically measurable via a two sided t-test $p \approx 8
\times 10^{-12}$.

Softwipe evaluated the \bigrig{} code to have a software score of 8.5, indicating a very high quality of software. 
This indicates that \bigrig{} is among the tools with the highest software quality.

\begin{figure}
    \centering
    \includesvg[width=\linewidth, pretex=\relscale{0.5}]{figs/bigrig.times.linreg.svg}
    \caption{Plot of smoothed generalized additive model (GAM) fit to runtime data for bigrig execution on a log-log
      scale.
      Plot was generated with \texttt{ggplot2}'s \texttt{geom\_smooth()} function.
      For each taxa quantity, 10 random rooted trees were generated via a Yule process with branch lengths scaled to a
      total tree hight of 1.0.
      For each tree, 10 sets of CMTC model parameters were randomly sampled such that $e, d \sim \mathrm{Uniform}(0,1)$.
      Cladogenesis parameters were set either to $s = v = y = 1.0$ and $j = 0.0$ or $s = v = y = 1.0$ (see text for
      details).
      Shaded area indicates the 0.95 confidence band as computed by the \texttt{predict.Gam} function in R.
    }\label{fig:runtime-taxa}
\end{figure}

\section{Conclusion}

We have presented \bigrig{}, a reliable, fast, and easy to use tool to generate simulated historical biogeography
datasets.
We have shown that \bigrig{} has a negligible chance of having a software implementation error.
We have also show that it is extremely fast, capable of generating a dataset in under a second which is far beyond the
ability of any current inference software to infer.

In the future, we wish extend the features of \bigrig{} to include dataset generation for the family of State Specific
Evolution (SSE) models (BiSSE, GeoSSE, ClaSSE) \citep{cornuault2022}.
The primary purpose of these extensions is to test the robustness of \decj{} models to model violations, in order to
asses the added value of these models.
This analysis is important for future tool development, as the more advanced SSE member models cannot be inferred using
the typical CMTC computational framework use for \decj{}.
However, at this time it seems like these models will exhibit similar scaling during inference as \decj{} as all models
here share the exponential scaling of their state space.

Investigating exactly when \decj{} fails to give adequate results will help both researchers and tool developers to
efficiently utilize their scarce resources.
In the case of users, it will help conserve compute time, and in the case of tool developers it will help conserve
developer time.

\section{Acknowledgements}

Many thanks to Alexander I. Jordan from the Computational Statistics group at the Heidelberg Institute for Theoretical
Studies for his assistance on deriving and implementing the statistical tests used to verify the results of \bigrig{}.

This work was funded by the European Union (EU) under Grant Agreement No. 101087081 (Comp-Bio-div-GR).
\begin{center}
  \includegraphics[width=0.25\textwidth]{figs/eu_logo.jpg}
\end{center}

\section{Data Availability}

The releases and code for \bigrig{} are available at \url{github.com/computations/bigrig}.
The scripts used to benchmark \bigrig{} and plot the results are available at \url{github.com/computations/bigrig-test}.

\bibliography{references}

\end{document}
