\documentclass{article}
\usepackage{geometry}[margins=1in]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[square, numbers, sort&compress]{natbib}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{relsize}

\bibliographystyle{abbrvnat}

\title{\texttt{bigrig}: A historical biogeography range simulator for the
	DEC[+J] and GeoSSE models}
\author{Ben Bettisworth}

\begin{document}

\newcommand{\CountFull}[1]{|#1|_\text{full}}
\newcommand{\CountEmpty}[1]{|#1|_\text{empty}}
\newcommand{\bigrig}{\texttt{bigrig}}
\newcommand{\rand}[2]{#1 \land #2}
\newcommand{\ror}[2]{#1 \lor #2}
\newcommand{\rneg}[1]{\neg #1}
\newcommand{\rxor}[2]{#1 \oplus #2}
\newcommand{\rLshift}[2]{#1 \ll #2}
\newcommand{\rRshift}[2]{#1 \gg #2}

\maketitle

\section{Introduction}

Verifying the correctness of software for computational phylogenetics is an ever
present challenge for tool developers \cite{carver_software_2007,
bettisworth_lagrange-ng_2023}. 
The true phylogenetic tree\textemdash the series of speciation events starting
from a common ancestor which explains an evolutionary history\textemdash is
almost always unobservable, rendering it impossible to verify the results
generated from software tools with a true phylogeny.
Occasionally, researchers will obtain or create known phylogenies
\cite{hillis_experimental_1992} which can be used to verify both models and
software. 
However, datasets for which the truth is known are exceptionally rare, with only
a few such datasets in existence.
Therefore, researchers cannot rely solely on these known phylogenies to
verify the correctness of their tools.

However, without software verification it is difficult to tell the difference
between a surprising result and an error in the software implementation (i.e. a
bug).
Responsible tool developers must therefore turn to artificially generated data
in order to verify the correctness of their programs \cite{ly-trong_alisim_2022,
fletcher_indelible_2009}.
Traditionally artificial data is generated by assuming some statistical model
as the true model, and generating data based on given model parameters.
While this data is unrealistic \cite{trost_simulations_2024}\textemdash failing to
capture the myriad complexities of real biological systems\textemdash it is
still invaluable for the purposes of software verification.

The field of historical biogeography has an analogous problem.
A common goal in historical biogeography is to infer the ancestral ranges
of species along a known phylogenetic tree 
\citep{varela_phylogeny_2019, baker_global_2013, vicente_and_2017}.
However, much like the true phylogenetic tree, these ancestral ranges are
generally not directly observable, and instead must be inferred from the fossil
record \cite{mclachlan_reconstructing_2004}.
But, even if fossils can be used to estimate ancestral ranges, there can be gaps
in the fossil record which make it difficult to be certain about the total
extent of ancestral ranges \cite{kidwell_quality_2002}.
All together, this means that a similar logic applies, and researchers must turn
to simulations in order to verify the correctness of their software which infers
the ancestral ranges.
To date, simulations of biogeographical data using the DEC[+J] model has often
been implemented in an ad hoc matter, with individual tools implementing their
own simulation framework \cite{matzke_statistical_2022,
bettisworth_lagrange-ng_2023}, even though tools which can perform simulations
exist already \citep{hohna_revbayes_2016}.
Sometimes, these simulations utilize the same code for as for the inference of
ancestral ranges and model parameters.
If this is the case, then there is the danger of 
a software error in the inference of the model which will in turn corrupt the
simulation results.

Rather than implementing an ad hoc simulation for each software project, a more
robust approach is to write a specialized software for simulation for a model
(or class of models).
There are four reasons for this that this ought to be preferred.
First, it is essential that computational tools are assessed against a common
standard.
A consistent standard for benchmarking enables meaningful comparisons of the
performance of these tools across different methods and implementations.
Without a consistent standard, it is unclear if improved performance is due to
the specific simulation, or to the quality of the software. 
Second, a specialised simulator reduces redundant work for tool developers
implementing software for the particular model.
A corollary is that a standard simulator reduces the amount of work required to
verify a new tool or features, and consequently reduces the human effort to
release a new tool.
This has the (eventual) result of increasing the number of tools available, and
therefore increasing the overall quality and diversity of tooling.
Third, an separation of simulation code and inference code reduces the chances
of a common error between the inference code and simulation code inadvertently
influencing performance.
In particular with the DEC[+J] model, the traditional method of inferring
ancestral parameters involves performing a matrix exponential, which has a
complexity of $\mathcal{O}(2^{3r})$ for $r$ regions\cite{bettisworth_lagrange-ng_2023}.
This operation is challenging, numerically unstable, and computationally
expensive.
If this same method is used to generate simulated datasets, then numerical
errors are likely \textit{undetectable} as the error between the methods is
consistent.
Fourth and finally, it is significantly easier to perform introspection on the
output of a well implemented simulator.
Ideally, a simulator will also provide detailed logs for all events that it
simulates.

To this end, we present the open source software \bigrig{}, a DEC[+J] simulator,
written in C++ which satisfies the four conditions described above.
\bigrig{} is extremely fast, capable of generating datasets for $\approx65000$
taxa and 63 regions in $\approx0.06$ seconds on a high end laptop.
In addition we show that the datasets generated by \bigrig{} are almost
certainly correct via a suite of very rigorous and extensive statistical tests.

\section{Background}

Before we describe the methods used to simulate and test the results generated
by \bigrig{}, we will define some terminology and notation.
A range will be written as a binary string, for example \texttt{0101}.
Each position in the string represents a region, with \texttt{1} denoting a full
region, and \texttt{0} denoting an empty region.
As the range is a string, the leading zeros are meaningful.
We denote the total number of regions for a range \( r \) as \( |r| = n\).
We write the number of full regions a region has as \( \CountFull{r} \), and the
number of empty regions as \( \CountEmpty{r} \).
We call a range a ``singleton" if $\CountFull{r} = 1$.
We denote a particular region in a range $r$ as $r_i$, for some index $i$.

Additionally, we define some operations on ranges.
We denote the \textit{bitwise} \textit{and}, \textit{or}, and \textit{exclusive
or} of ranges $r$ and $s$ as $\rand{r}{s}$, $\ror{r}{s}$, $\rxor{r}{s}$
respectively.
We denote the \textit{bitwise} \textit{right shift} and \textit{left shift} of a
range $r$ and a positive integer $n$ as $\rLshift{r}{n}$ and $\rRshift{r}{n}$.
The \textit{bitwise negation} is denoted by $\rneg{r}$.
\textit{Bitwise} indicates that the operation should be performed
region by region. 
For example the \textit{bitwise and} is defined as detail $r_i \land s_i = t_i$
where $0 \leq i \leq |r|$, $\texttt{1} \land \texttt{1} = \texttt{1}$ and is
equal to \texttt{0} otherwise.

These operations are of a particular interest for us as they have either direct
analogues as CPU instructions, or can be trivially computed using a composition
of CPU instructions.
Consequently, computing the results of these operations only requires 1 or 2 CPU
cycles. 
As such, computation of these operations (and functions which are composed of
these operations) is extremely efficient.

\subsection{A Summary of the DEC[+J] Model and its Variants} \label{sec:model}

The Dispersion, Extinction, and Cladogenesis (DEC) model defines the probability
of observing particular geographical history given a pair of parameters for
dispersion and extinction rates and a phylogenetic tree equipped with branch
lengths. 
In this model, there are 3 processes affecting the range of along a phylogenetic
tree.
The first two processes, dispersion and extinction, represent the stochastic
range shifts over time, either newly occupying or vacating a region.
The process is modeled as each region independently transitioning to a
different state (either from full to empty or vise versa) with waiting times
distributed exponentially based on the respective rate.
This is to say, for a region \( r_i \) which is full with extinction
rate \( e \) the waiting time \( w \) for \( r_i \) to transition to empty is \(
w \sim \text{Exp}(e) \) (or $w \sim \text{Exp}(d)$ to transition an empty region
to a full region).

The third process, cladogenesis, occurs when a parent species splits into two
daughter species.
The ranges of the two daughter species are inherited from the parent's range,
and are therefor a subset of the parent range.
DEC defines a set of three scenarios intended to represent realistic speciation
events.
In \citet{ALikelihoodFrReeR2005}, these scenarios are simply labeled Scenario 1,
2, and 3, which represent ``copy" (single-range sympatric), allopatric, and
sympatric events, respectively.
In the interest of clarity, we will eschew using the original labels, and
instead use the terms \textit{copy}, \textit{allopatry}, and \textit{sympatry}
to describe speciation scenarios similar to \citet{ModelSelectionMatzke2014}.
In addition to the three original cladogenesis types 
\citet{ModelSelectionMatzke2014} proposed a ``jump" type, intended to represent
``founder-event speciation", where a small population becomes isolated via
colonization of a new area.
We give a formal definition for the cladogenesis events in the following Section.

\subsection{A Formal Definition of Cladogenesis Events}
\label{sec:formal-cladogenesis}

A cladogenesis event can be thought of as a tuple of ranges $(p, l, r)$, where
$p$ is the parent range, and $l$ and $r$ are the left and right child ranges,
respectively. 
An assumption by the DEC[+J] model is that speciation can only occur in one
area, and therefore at least one of the daughter ranges ($l$ or $r$) will be a
singleton.
For the rest of this section, let us suppose that the singleton range is $l$,
without loss of generality.

When a cladogenesis event occurs the \textit{strict} DEC model assumes the
probability of these event types is equal.
This is to say, when considering if a particular cladogenesis result was the
an allopatric or sympatric, \textit{strict} DEC considers these events to
have equal weight.
To compute the probability of a particular type of cladogenesis event over the
set \(T = \left\{\text{Allopatry, Sympatry, Copy}\right\}\) we compute
\[
	P(t |
	s) = \frac{C(t | p)}{\sum_{u \in T} C(u | p)},
\]
where \( C(t|p) \) is the
count of cladogenesis events of type $ t $ which are possible given the parent
range.

Matzke\cite{ModelSelectionMatzke2014} extended the cladogenesis probability
computation by allowing for a new ``jump" type of cladogenesis events and
introducing weights for each of the different cladogenesis types such that a
particular event type can be more likely than the others.
This pair of modifications to the DEC model are generally denoted as DEC+J.
Under this modified model, the probability of a particular type of cladogenesis
event is computed as
\begin{equation}
	P(t | s) = \frac{w_t C(t | s)}{\sum_{u \in T} w_u C(u |
		s)},
\end{equation}
where $w_t$ is the weight for the cladogenesis type $t$.
Additionally, $T$ is augmented by the new type ``jump" to become \(T =
\left\{\text{Allopatry, Sympatry, Copy, Jump}\right\} \).
Informally, ``jump" events are cladogenesis events which allows one of the
daughter species to disperse to a unoccupied range.

We define the cladogenesis event types as
\begin{equation}
T(e) = T(p, l, r) = 
\begin{cases}
  \text{Copy} & \text{if }l = r = p \\ 
  \text{Sympatric} & \text{if } \CountFull{\rand{l}{r}} = 1 \text{ and } \ror{l}{r} = p \\ 
  \text{Allopatric} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } \ror{l}{r} = p   \\ 
  \text{Jump} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } r = p. \\ 
\end{cases}
\label{eq:clad-test}
\end{equation}
Importantly, this definition is in terms of bitwise operations and equality
operations.

\subsection{Additional Extensions to the DEC Model}

\section{Methods}

For the purposes of generating data under the DEC[+J] model, there are two
phases to the simulation: range changes \textit{along} a branch(dispersion and
extinction), and range changes at speciation events (cladogenesis).
For the purpose of memnotics, we call these two phases the \textit{spread} and
the \textit{split}.

For both phases, we have implemented two independent methods of simulating
results, with the purpose of checking that the results of the two methods match.
Specifically, we have implemented independent methods with the goal of ensuring
identical results between the two.
The first method of simulating results (for both the spread and split phases) is
rejection sampling.
In rejection sampling, a random result is generated and then checked for
validity.
If the result is valid, it is accepted (possibly with some probability to
correct for the initial distribution).
If the result was invalid, it is rejected and a new result is generated.
In general, rejection methods are very computationally inefficient, as some of
the results generated are simply discarded. 
However, they are extremely reliable and easy to implement.
This is to say, they have a low chance of software errors.

In addition to the rejection method implementations, we additionally implemented 
``fast" methods.
These ``fast" methods implement two major categories of optimizations: analytic
and operational.
Here, an analytic optimization is where we derive a fast algorithm which
analytically ought to generate the correct distribution of results.
On the other hand, an operational optimization is where we implement the
algorithm using efficient operations.
However, just because an algorithm is analytically correct, does not mean it is
\textit{numerically} correct\cite{goldberg_what_1991, noauthor_ieee_1985}.

The other methods of simulating results we have implemented are analytic
methods.
These methods are analytically derived from, and are composed of, operations on
``off the shelf" statistical distributions (E.g. Uniform, Normal, Exponential).
That is, we find an algorithm which ought to generate the correct distribution
of results.
We discuss the details of these optimizations later.

With the independent implementations of both rejection and ``fast" methods, we
can use them to check the results of each other.
That is, by establishing that the results of the two methods are equivalent, in
a rigorous statistical sense, we can establish that if there is a software bug,
it must be present in both methods.
However, the probability of the same software bug being present in both
independent and markedly different implementations is extremely low.
Therefore, if the results of the two methods are statistically equivalent, we
can be extremely confident that the results are correct.

\subsection{Simulating the Spread}

As mentioned in Section~\ref{sec:model}, the evolution of a range along a
branch is modeled as a Poisson process, analogous to the character evolution
models used in phylogenetic inference.

The problem is this: given an initial range \( r \) with \( |r| = n \) regions,
and a branch length \( t \), produce a final range which has undergone
extinction and dispersion at rates \( e \) and \( d \), respectively.
When a range experiences dispersion, it gains a region, and when a range
experiences extinction, it loses a region. 
Each of the regions is treated as an independent process from each other.
This observation is the basis for the rejection method of spread simulation:
randomly generate \( n \) waiting times, labeled \( w_i \), for each region.
Specifically, the waiting times are distributed as
\begin{equation}
	\label{eq:exp-rejection} w_i \sim
	\begin{cases}
		\text{Exp}(e) & \text{if } r_i
		= 1                            \\ \text{Exp}(d) & \text{if } r_i = 0.
	\end{cases}
\end{equation}
Once all $n$ waiting times are generated, we pick the smallest, say $w_i$, and
negate $r_i$.
We repeat this process until the total waiting time exceeds the branch length,
at which point we return a list of transition events, excluding the final event
who's waiting time exceeded the branch length.

Keen readers might already be aware of the fact that if we have a set of
independent processes like above, the entire set can be represented by a single
exponential distribution, with \(w \sim \text{Exp}(t) \) where
\begin{equation}
	\label{eq:exp-param} t = e \times \CountFull{r} + d \times \CountEmpty{r}.
\end{equation}
This second method then is to compute \( t \) using Eq.~\ref{eq:exp-param},
draw a waiting waiting time \( w \sim \text{Exp}(t) \).
Once a time is rolled, we pick a region with weights equal to the exponential
distribution parameter in Eq.~\ref{eq:exp-rejection}.

We implement both of the above methods, but only use the second to preform
simulations.
The first method is used to check the results of the second method.

\subsection{Simulating the Split}

Note that a split can be viewed as an ordered triplet of binary numbers \(
(p,l,r) \), where $p$ is the parent range and $l$ and $r$ are the child ranges.
Therefore, we can simulate a split by generating two random numbers (the parent
split $p$ is given), and rejecting invalid splits.
That is, we accept splits which fall into one of the categories defined in
Section~\ref{sec:model}.
We present a C++ function to determine the split type given three
\mintinline{c++}{uint64_t} (labeled \mintinline{c++}{dist_t}) in
Listing~\ref{lst:determine-split-type}.
Using this function, we reject any split that is of type
\mintinline{c++}{split_type_e::invalid}, and accept otherwise.

In order to implement the weighted split type introduced by Matzke in
\cite{ModelSelectionMatzke2014}, we accept with probability equal to the
normalized split weight.
For example, if the split type weights are $y = 1.0, s = 1.0, v = 1.0, j=1.0$,
then a jump type split will be accepted with probability $j/(y + s + v +
j)$.

To accelerate this process, we implement an optimized version of split
simulation.
First, we generate the split type according to the type weights.
This is done in two cases, singleton and non-singleton.
In the singleton case, we generate a split type from $\{\text{jump},
\text{copy}\}$, weighted accordingly.
In the non-singleton case, we generate a split type from $\{\text{jump},
\text{sympatry}, \text{allopatry}\}$, also weighted accordingly.

The total weight for each type is
\begin{equation}
	w_s = C(s|r) \times u_s
\end{equation}
where $C(s|r)$ is the count of splits of type $s$ that are
possible given region $r$.
For example, for the region \texttt{11} there are 2 ways to split
allopatrically: $(\texttt{10}, \texttt{01})$ and $(\texttt{01},
\texttt{10})$\footnote{Left and right branches are distinguishable, so swaps
	are also valid.}\footnotemark.
Occasionally, it is convenient to refer to the normalized split weight, in
which case we write \( \overline{w_s} \).
The formulas for various counts are
\begin{align*}
  C(\text{allopatry}|r) & =
  \CountFull{r} \times 2 - (2 \text{ if } \CountFull{r} = 2) \\
  C(\text{sympatry}|r)  & = \CountFull{r} \times 2           \\ C(\text{jump}|r) & =
  \CountEmpty{r} \times 2                                    \\ C(\text{copy}|r) & = 2 \\
\end{align*}

\footnotetext{
  This result depends on the process used to count.
	If an region is selected, and then the child to inherit that region is
	selected, you get 4 ways to allopatrically split with 2 full regions.
	That is, a process based counting method gets a different result, where the
	process is a speciation event in a region, and then a random selection of the
	child branch.}

Once a split type is simulated, we generate
a split according to that type by first determining which region will be
flipped\footnotemark.
If the split type is jump, the region is chosen from among the empty regions.
If the split type is allopatry or sympatry, the region is chosen from among the
full regions.
Once the region is flipped, we pick randomly which child gets which region,
with uniform probability.

\footnotetext{If the type is copy, we can simply return the tuple $(p, p, p)$
where $p$ is the parent range.}

\begin{listing}
	\begin{minted}{c++}
split_type_e determine_split_type(dist_t parent_dist, 
                                  dist_t left_dist, 
                                  dist_t right_dist) {
  size_t diff_region_count =
         (left_dist & right_dist).full_region_count();

  if ((left_dist | right_dist) == parent_dist) {
    if (left_dist == right_dist 
        && left_dist.full_region_count() == 1) {
      return split_type_e::singleton;
    }

    if (!left_dist.singleton() 
        && !right_dist.singleton()) {
      return split_type_e::invalid;
    }

    if (diff_region_count == 1) {
      return split_type_e::sympatric;
    }

    if (diff_region_count == 0) {
      return split_type_e::allopatric;
    }
  } else if (left_dist.singleton()
             && diff_region_count == 0
             && right_dist == parent_dist) {
    return split_type_e::jump;
  }
  return split_type_e::invalid;
}
\end{minted}
	\caption{A function to determine the split type given three numbers.}
	\label{lst:determine-split-type}
\end{listing}

\subsubsection{Simulating a Phylogenetic Tree}


\subsection{Testing}

\subsubsection{Spread}

For the purposes of ensuring correct results, we perform 2 types of tests.
The first test pre-computes the expected value of the waiting time given the
model parameters, and performs a t-test against that expected value.
We compute 1,886,084,219 iterations of the spread function, which allows us to
be 99.999\% confident that the error is less than 0.0001.
We perform this test for 7 different regions and 16 different parameter sets.

The second test is a regression test against the rejection method.
We again run both methods for 1,886,084,219 iterations, which again ensures
that the error is less than 0.0001 with 99.999\% confidence.
We perform this test for 5 different initial regions and 16 different parameter
sets.

\subsubsection{Split}

To test that the correct proportion of splits are generated by the simulation,
we simulate a sample of splits, and check that the proportion of generated
splits matches the expected proportion.
Specifically, we check that if there are \(n\) simulations for a given model
and range \( r \), then we should expect that there are \(\overline{w_t} \times
n\) splits of type \( t \).
We model the realized count of split types as being drawn from a normal
distribution, and perform a t-test to ensure that the realized count is
approximately equal to the expected count.
Again, we compute 1,886,084,219 simulations for 10 different ranges and 8
different sets of model parameters, which ensures a 99.999\% confidence that
the error is less than 0.0001.

Like with the spread, we use the slower rejection method to test that the
results of the optimized method are correct.
We run both methods for 487,791,396 iterations and count of generated types for
each.
This number of iterations ensures that the methods produce the same results to
a tolerance of \texttt{1e-4} with 99.999\% confidence.
We perform this test for 5 different parent regions, and 5 different sets of
model parameters.

\section{Results}

\begin{figure}
    \centering
    \includesvg[width=\linewidth, pretex=\relscale{0.5}]{figs/bigrig.times.linreg.svg}
    \caption{Runtime plot}\label{fig:runtime-taxa}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=\linewidth, pretex=\relscale{0.5}]{figs/bigrig.times.boxplot.svg}
    \caption{Runtime plot}\label{fig:runtime-regions}
\end{figure}

\section{Conclusion}

\section{Acknowledgements}

Many thanks to Alexander I. Jordan for his assistance on deriving and
implementing the statistical tests used to verify the results of \bigrig{}.

\bibliography{references}

\end{document}
