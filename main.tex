%! TeX program = lualatex

\documentclass[a4paper]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{geometry}[margins=1in]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[square, numbers, sort&compress]{natbib}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{relsize}
\usepackage{todonotes}

\bibliographystyle{abbrvnat}

\title{\texttt{bigrig}: A historical biogeography range simulator for the
	DEC[+J] and GeoSSE models}
\author{Ben Bettisworth}

\newcommand{\CountFull}[1]{|#1|_\text{full}}
\newcommand{\CountEmpty}[1]{|#1|_\text{empty}}
\newcommand{\bigrig}{\texttt{bigrig}}
\newcommand{\decj}{DEC[+J]}
\newcommand{\rand}[2]{#1 \land #2}
\newcommand{\ror}[2]{#1 \lor #2}
\newcommand{\rneg}[1]{\neg #1}
\newcommand{\rxor}[2]{#1 \oplus #2}
\newcommand{\rLshift}[2]{#1 \ll #2}
\newcommand{\rRshift}[2]{#1 \gg #2}

\begin{document}

\maketitle

\section{Introduction}

Verifying the correctness of software for computational phylogenetics is an ever present challenge for tool developers
\cite{darribaStateSoftwareEvolutionary2018, mendesHowValidateBayesian2025, carver_software_2007,
bettisworth_lagrange-ng_2023}. 
The true phylogenetic tree\textemdash the series of speciation events starting from a common ancestor which explains an
evolutionary history\textemdash is almost always unobservable, rendering it impossible to verify the results generated
from software tools with a true phylogeny.
Occasionally, researchers will obtain or create known phylogenies \cite{hillis_experimental_1992} which can be used to
verify both models and software. 
However, datasets for which the truth is known are exceptionally rare, with only a few such datasets in existence.
Therefore, researchers cannot rely solely on these known phylogenies to verify the correctness of their tools.

Nonetheless, software verification remains vital, as without verification it is difficult to tell the difference between
a surprising result and an erroneous result which is the result of a software error.
Responsible tool developers must therefore turn to artificially generated data in order to verify the correctness of
their programs \cite{mendesHowValidateBayesian2025, ly-trong_alisim_2022, fletcher_indelible_2009}.
Generally, artificial data is generated by assuming some statistical model as accurate, and generating data based on
a given set of model parameters.
Data generated this way is unrealistic as it fails to capture the myriad complexities of real biological systems
\cite{trost_simulations_2024}.
Nonetheless, the use of artificial data is still ubiquitous when verifying software as there exists no real alternative.

The field of historical biogeography has an analogous issue.
A common goal in historical biogeography is to infer the ancestral ranges of species along a known phylogenetic tree
\citep{varela_phylogeny_2019, baker_global_2013, vicente_and_2017}.
However, much like the true phylogenetic tree, these ancestral ranges are generally not directly observable.
They might be inferred from the fossil record \cite{mclachlan_reconstructing_2004}, but there can be multiple
complications which limit knowledge of the true ancestral ranges.
These complications include, but are not limited to: gaps in the fossil record; the study system includes soft bodied
organisms; or a geographical distribution which is unsuited for fossilization \cite{kidwell_quality_2002}.
All together, this indicates that researchers do not have a reliable source of known ancestral ranges and associated
phylogenetic trees, and therefore must turn to simulations in order to verify the correctness of their software tools.
To date, simulations of biogeographical data using the DEC[+J] model has often been implemented in an ad hoc matter,
with individual tools implementing their own simulation framework \cite{matzke_statistical_2022,
bettisworth_lagrange-ng_2023}, even though tools which can perform simulations exist \cite{hohna_revbayes_2016}.
Sometimes, these simulations utilize the same code for as for the inference of ancestral ranges and model parameters.
If this is the case, then there is the danger of a software error in the inference of the model which will in turn
corrupt the simulation results.

Rather than implementing an ad hoc simulation for each software project, a better solution is to write a bespoke
implementation of simulation software for a model (or class of models).
There are several important reasons for this.
First, it is critical that tools which implement inference for a specific model are assessed using a common standard.
In ensuring that a common standard is used to evaluate tools, researchers utilizing tools can be sure that high
performance is not simply a consequence of the evaluation process.

Second, a common simulation tool drastically reduces the amount of duplicated work for tool developers.
A corollary is that a common simulator lowers the amount of work required to implement a new tool, as a high
quality method of generating testing data can be utiliized by developers of novel tools.
This will have the (eventual) result of increasing the number of tools available, and will increase the overall quality
of tooling.

Third, a separation of simulation code and inference code reduces the chances of integrated bugs in both the simulation
and the inference, which might improve apparent performance at the expense of actual performance.
This is to say, if there is an error in a shared region of code between inference and simulation, the error could be
masked, as the simulation step (erroneously) agrees with the inference step.

Fourth and finally, it is significantly easier to perform introspection on the results of a well implemented simulator.
Specifically, events which are typically marginalized over in likelihood models are instead explicitly simulated, and
therefore can recorded for later analysis.
For example, a state change along the branch of a phylogenetic tree can be added to a log.
This explicit logging of results ensures the transparency of results, which in turn ensures that the resulting data are
explainable.
This in turn ensures that the bugs \textit{in the simulation tool} are faster and simpler to find, as results which are
erroneous will have erroneous explanations.
Records of intermediate states and marginalized parameters also enables future tools to implement and test inference of
these parameters without any further modification to the simulation tool.

In this paper we present \bigrig{}, a simulator for the \decj{} \todo{And possibly a limited version of GeoSSE} model
which meets the above criteria for a simulator. For a full explanation of the \decj{} model, please see
Section~\ref{sec:model}.
Given a set of \decj{} parameters and a phylogenetic tree, \bigrig{} will generate a set of ranges for the taxa at the
tips of the tree.
In addition, ranges for inner nodes, cladogenesis events, and state transitions along the branch will be generated.
After generation, \bigrig{} will log the results in one of a variety of formats (YAML, JSON, or CSV), as well as writing
the results in a phylip format, and an annotated tree in newick format.

Furthermore, we show that \bigrig{} is both extremely reliable and extremely performant.
We show that \bigrig{} is reliable using two separate methods.
The first is by deriving expected distributions for the results of fundamental model events, and using statistical
testing ensure the results are within 0.0001 with 99.999\% of type 1 or type 2 error.
The second is to perform the simulation with an independent implementation using a slower, but simpler to implement
method, and performing statistical tests to verify that the results of the two methods agree.
Finally, we show that \bigrig{} is performant by showing that it can generate datasets for trees with tens of thousands
of tips and 63 regions in under 0.5 \todo{check the time to be sure} seconds using a mid-range laptop.

\section{Background}

For this work, we define the individual sectors that a taxon can be found is referred to as a \textit{region}.
An \textit{area} is made up of non-overlapping regions, and all regions together make up the complete area under study.
When not otherwise stated, we will use area to mean the complete area.
A \textit{range} describes the absence or presence for every region in an area.
We denote the total number of regions for a range \( r \) as \( |r| \).
Each region can either be occupied, in which case we say it is full, or it can be unoccupied, in which case we say it is
empty.
We write the number of full regions a range has as \( \CountFull{r} \), and the number of empty regions as \(
\CountEmpty{r} \).
The value of the $i$th region of a range will be denoted as $r_i$.
In practice and for simplicity, ranges will be represented as binary strings, with $1$ indicating a full region, and $0$
representing an empty region.
\todo{Maybe an example?}

For the regions $r$ and $s$ we denote \textit{bitwise and} as $\rand{r}{s}$, \textit{bitwise or} as $\ror{r}{s}$,
\textit{bitwise exclusive or} as $\rxor{r}{s}$, and \textit{bitwise negation} as $\rneg{r}$.
Here, \textit{bitwise} indicates that the operation should be independently on each character in the string (that is,
each region). 
For example the \textit{bitwise and} is defined as detail $r_i \land s_i = t_i$ where $0 \leq i \leq |r|$, $\texttt{1}
\land \texttt{1} = \texttt{1}$ and is equal to \texttt{0} otherwise.
Additionally, we denote the non-rotating \textit{right shift} as $\rRshift{r}{n}$, and \textit{left shift} as
$\rLshift{r}{n}$, for some positive integer $n$.

These operations are of a particular interest as they can be computed using only trivial CPU instructions.
Consequently, computing the results of these operations often requires less than a single CPU cycle\footnotemark
\citep{Abel19a}.
As such, computation of these operations (and functions which are composed of these operations) is extremely efficient.

\footnotetext{This is due to the capabilities of modern CPUs to dispatch more than one instruction per cycle, depending
on the specific instructions.}

\subsection{A Summary of the DEC model and its Variants} \label{sec:model}

The Dispersion, Extinction, and Cladogenesis (DEC) model defines the probability of observing a biogeographical
history along a given phylogenetic tree.
In this model, there are 3 processes which govern range evolution.
The first two processes, dispersion and extinction, represent the stochastic range shifts over time, either newly
occupying or vacating a region.
The process is modeled as each region independently transitioning to a different state (either from full to empty or
vise versa) with waiting times distributed exponentially based on a parameterized rate.
This is to say, for a region \( r_i \) which is full with extinction rate \( e \) the waiting time \( w \) for \( r_i \)
to transition to empty is \( w \sim \text{Exp}(e) \), or $w \sim \text{Exp}(d)$ to transition an empty region to a full
region.

The third process is cladogenesis, I.E. when a parent species splits into two daughter species.
During this, some regions from the parent's range will be inherited by one daughter, and rest will be inherited by the
other.
The particular way in which the parent ranges are inherited is restricted to a limited set of scenarios.
In the original Ree \textit{et. al.}\cite{ALikelihoodFrReeR2005} paper, these scenarios are simply given as Scenario 1,
2, and 3.
These scenarios are intended to represent realistic cladogenesis methods, specifically allopatry, and sympatry, with an
additional scenario added for single region cladogenesis event.
In the interest of clarity and memorability, we will use the more common names, similar to how
how it is described in \citet{ModelSelectionMatzke2014}.

Informally, the scenarios possible in the strict DEC model are: allopatry (alternatively, vicariance), where the
daughter ranges are disjoint; sympatry, where the daughter species share some at least one range; and copy, where both
daughter ranges are the same as the parent's singluar range.
Copy event is distinct from sympatric event as it can only occur when the parent range consists of only a single region
(a singleton).
We give a formal definition of cladogenesis events in Section~\ref{sec:formal-cladogenesis}.

\citet{ModelSelectionMatzke2014} extended the set of cladogenesis events by adding a new ``jump" type, intended to
represent ``founder-event speciation", where a small population becomes isolated via colonization of a novel region.
Additionally, \citet{ModelSelectionMatzke2014} also allowed for the relative probability of the cladogenesis events to
vary.
Under this extension, sympatric events might have a larger weight, and therefore have a higher likelihood, than
allopatric events.
This pair of extensions are generally denoted as DEC+J, similar to how gamma rate categories are denoted as GTR+G4 in
phylogenetic models.

The parameters for the strict DEC model are rates for dispersion and extinction ($d$ and $e$), and a phylogenetic tree
with branch lengths. 
\decj{} adds the cladogenesis parameters $s$, $v$, $y$ and $j$, for sympatric, allopatric (vicariance), copy, and jump
events respectively.

\subsection{A Formal Definition of Cladogenesis Events}
\label{sec:formal-cladogenesis}

We will represent a cladogenesis event as a tuple $(p, l, r)$, where $p$ is the parent range, $l$ is the left child
range, and $r$ is the right child range.
As an assumption by the DEC[+J] model, speciation occurs instantaneously and isolated to a single region.
As such, at least one of the daughter ranges ($l$ or $r$) will be a singleton.
For the rest of this section we will assume that $l$ is the singleton range, without loss of generality.

When a cladogenesis event occurs the \textit{strict} DEC model assumes equal probability for all event types.
This is to say, when considering if a particular cladogenesis result is more likely to be allopatric or sympatric,
\textit{strict} DEC considers these events to have equal weight.
To compute the probability of a type of cladogenesis event over the set \(T = \left\{\text{Allopatry, Sympatry,
Copy}\right\}\) we compute
\[
	P(t |
	s) = \frac{C(t | p)}{\sum_{u \in T} C(u | p)},
\]
where \( C(t|p) \) is the count of cladogenesis events of type $t$ which are possible given the parent range, and $T$ is
the set of all possible types.

The cladogenesis model is extended in \decj{} to include weights and an additional cladogenesis type.
With this extended model, the probability computation becomes
\begin{equation}
	P(t | p) = \frac{w_t C(t | p)}{\sum_{u \in T} w_u C(u | p)},
\end{equation}
where $w_t$ is the weight for the cladogenesis type $t$.
Additionally, $T$ is augmented by the new type ``jump" to become \(T = \left\{\text{Allopatry, Sympatry, Copy,
Jump}\right\} \).
Informally, ``jump" events are cladogenesis events which allows one of the daughter species to disperse to a unoccupied
range.

We define the cladogenesis event type as
\begin{equation}
T(e) = T(p, l, r) := 
\begin{cases}
  \text{Copy} & \text{if }l = r = p \\ 
  \text{Sympatric} & \text{if } \CountFull{\rand{l}{r}} = 1 \text{ and } \ror{l}{r} = p \\ 
  \text{Allopatric} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } \ror{l}{r} = p   \\ 
  \text{Jump} & \text{if } \CountFull{\rand{l}{r}} = 0 \text{ and } r = p. \\ 
\end{cases}
\label{eq:clad-test}
\end{equation}

\subsection{Additional Extensions to the \decj Model}

A common practice during \decj{} analyses is include information about the ease of dispersions or jumps occurring
between two areas.
This information normally takes the form of a connectivity graph, which represents the connectivity of two regions as a
positive number.
Often, researchers derive the pairwise region connectivities from the geographic distance between the two regions.
The dispersion rate and jump weight between two regions $i$ and $j$ are then modified using the provided connectivities
as
\begin{align*}
  d_{ij} &= d c_{ij}^w \\
  j_{ij} &= j c_{ij}^w,
\end{align*}
where $w$ is an inferred parameter and $c_{ij}$ is the user supplied connectivity between $i$ and $j$.

Some other authors might make a distinction between connectivity and distance matrices.
However, whether a connectivity matrix or a distance matrix is used, the operation is the same.
The only difference is the power of the exponent $w$: positive for connectivity matrices, and negative for
distance matrices.
Given that they represent the same concept, we will not make a distinction between distance and connectivity matrices.

\section{Methods}

For the purposes of generating data under the DEC[+J] model, there are two phases to the simulation: range changes which
occur along a branch (dispersion and extinction), and range changes which occur at speciation events (cladogenesis).
For mnemonics, we will refer to these phases as the \textit{spread} and \textit{split} phases.

For both phases, we have implemented two independent methods of data simulation, with the goal of checking that the
results of the two methods match. 
The first method of simulating results (for both the spread and split phases) is based on the pattern of rejection
sampling.
Rejection sampling is a method where the results of a distribution over a complicated sample (say $\Omega$) space is produced by
first sampling from a distribution over a simpler sample space (say $\Omega'$). 
If a sample drawn from $\Omega'$ is not in $\Omega$, then the sample is rejected as invalid, and the sample is drawn
again.
This process is repeated until a valid sample is drawn.
Rejection sampling can be further modified such that non-uniform distributions are produced by rejecting drawn samples
some proportion of the time in order to correct the bias.
The precise probability of rejection in this case is dependant on the specific distributions, so we will specify that
probability in detail when we discuss each method individually.

Rejection methods are extremely simple, and therefore are easy to implement correctly.
The trade-off is that a substantial amount of time is spent on generating rejected samples, resulting in very
computationally inefficient implementations.
As such, they are typically only utilized when no other method capable of producing acceptable results.

In addition to the rejection methods detailed above, we implemented so-called ``fast" methods.
These ``fast" methods implement two major categories of improvements: analytic and CPU-aware optimizations.
Here, an analytic optimization is where we derive a fast algorithm which analytically ought to generate the correct
distribution of results.
On the other hand, CPU-aware optimizations are when we implement the algorithm using operations which take advantage of
the computational capabilities of the CPU.
However, the analytically correctness of an algorithm does not imply that it is \textit{numerically} correct
\cite{goldberg_what_1991, noauthor_ieee_1985}.

With the independent implementations of both rejection and ``fast" methods, we can use them to check the results of each
other.
That is, by establishing that the results of the two methods are equivalent, in a rigorous statistical sense, we can
establish that if there is a software bug, it must be present in both methods.
However, the probability of the same software bug being present in both independent and markedly different
implementations is extremely low.
Therefore, if the results of the two methods produce statistically equivalent results, we can be confident that the
results are correct.

\todo[inline, color=green!20]{The text has been edited up to here. Beyond this is a rough draft.}

\subsection{Simulating the Spread}

A transition event is the tuple $(p, c, w)$, where $p$ is the parent range, $c$ is the child range, and $w$ is the
waiting time between $p$ and $c$.
As mentioned in Section~\ref{sec:model}, the evolution of a range along a branch is modeled as a Poisson process.
The problem is: given an initial range \( r \) with \( |r| = n \) regions and branch length \( t \), produce a final range which
has undergone the processes of extinction and dispersion at rates of \( e \) and \( d \), respectively.
Each of the regions is treated as an independent process from each other.
This observation is the basis for the rejection method of spread simulation: randomly generate \( n \) waiting times,
labeled \( w_i \), for each region.
Specifically, the waiting times are distributed as
\begin{equation}
	\label{eq:exp-rejection} w_i \sim
	\begin{cases}
		\text{Exp}(e) & \text{if } r_i = 1 \\
    \text{Exp}(d) & \text{if } r_i = 0.
	\end{cases}
\end{equation}
Once all $n$ waiting times have been generated, we find $ i = \text{argmin}(w_i)$ and negate the corresponding
region so that $r_i' := \rneg{r_i}$.
The resulting transition event is then $(r, r', w_i)$.

We repeat the above process until the total waiting time exceeds $t$, the sum of selected $w_i$'s, at which point the
process halts, and we yield the list of transition events, excluding the final generated event.

Keen readers might already be aware of the fact that if we have a set of
independent processes like above, the entire set can be represented by a single
exponential distribution, with \(w \sim \text{Exp}(t) \) where
\begin{equation}
	\label{eq:exp-param} t = e \times \CountFull{r} + d \times \CountEmpty{r}.
\end{equation}
This second method then is to compute \( t \) using Eq.~\ref{eq:exp-param},
draw a waiting waiting time \( w \sim \text{Exp}(t) \).
Once a time is rolled, we pick a region with weights equal to the exponential
distribution parameter in Eq.~\ref{eq:exp-rejection}.\todo{Maybe this should come first?}

We implement both of the above methods, but only use the second to preform
simulations.
The first method is used to check the results of the second method.

\subsection{Simulating the Split}

Note that a split can be viewed as an ordered triplet of binary numbers \(
(p,l,r) \), where $p$ is the parent range and $l$ and $r$ are the child ranges.
Therefore, we can simulate a split by generating two random numbers (the parent
split $p$ is given), and rejecting invalid splits.
That is, we accept splits which fall into one of the categories defined in
Section~\ref{sec:model}.
We present a C++ function to determine the split type given three
\mintinline{c++}{uint64_t} (labeled \mintinline{c++}{dist_t}) in
Listing~\ref{lst:determine-split-type}.
Using this function, we reject any split that is of type
\mintinline{c++}{split_type_e::invalid}, and accept otherwise.

In order to implement the weighted split type introduced by Matzke in
\cite{ModelSelectionMatzke2014}, we accept with probability equal to the
normalized split weight.
For example, if the split type weights are $y = 1.0, s = 1.0, v = 1.0, j=1.0$,
then a jump type split will be accepted with probability $j/(y + s + v +
j)$.

To accelerate this process, we implement an optimized version of split
simulation.
First, we generate the split type according to the type weights.
This is done in two cases, singleton and non-singleton.
In the singleton case, we generate a split type from $\{\text{jump},
\text{copy}\}$, weighted accordingly.
In the non-singleton case, we generate a split type from $\{\text{jump},
\text{sympatry}, \text{allopatry}\}$, also weighted accordingly.

The total weight for each type is
\begin{equation}
	w_s = C(s|r) \times u_s
\end{equation}
where $C(s|r)$ is the count of splits of type $s$ that are
possible given region $r$.
For example, for the region \texttt{11} there are 2 ways to split
allopatrically: $(\texttt{10}, \texttt{01})$ and $(\texttt{01},
\texttt{10})$\footnote{Left and right branches are distinguishable, so swaps
	are also valid.}\footnotemark.
Occasionally, it is convenient to refer to the normalized split weight, in
which case we write \( \overline{w_s} \).
The formulas for various counts are
\begin{align*}
  C(\text{allopatry}|r) & =
  \CountFull{r} \times 2 - (2 \text{ if } \CountFull{r} = 2) \\
  C(\text{sympatry}|r)  & = \CountFull{r} \times 2           \\ C(\text{jump}|r) & =
  \CountEmpty{r} \times 2                                    \\ C(\text{copy}|r) & = 2 \\
\end{align*}

\footnotetext{
  This result depends on the process used to count.
	If an region is selected, and then the child to inherit that region is
	selected, you get 4 ways to allopatrically split with 2 full regions.
	That is, a process based counting method gets a different result, where the
	process is a speciation event in a region, and then a random selection of the
	child branch.}

Once a split type is simulated, we generate
a split according to that type by first determining which region will be
flipped\footnotemark.
If the split type is jump, the region is chosen from among the empty regions.
If the split type is allopatry or sympatry, the region is chosen from among the
full regions.
Once the region is flipped, we pick randomly which child gets which region,
with uniform probability.

\footnotetext{If the type is copy, we can simply return the tuple $(p, p, p)$
where $p$ is the parent range.}

\begin{listing}
	\begin{minted}{c++}
split_type_e determine_split_type(dist_t parent_dist, 
                                  dist_t left_dist, 
                                  dist_t right_dist) {
  size_t diff_region_count =
         (left_dist & right_dist).full_region_count();

  if ((left_dist | right_dist) == parent_dist) {
    if (left_dist == right_dist 
        && left_dist.full_region_count() == 1) {
      return split_type_e::singleton;
    }

    if (!left_dist.singleton() 
        && !right_dist.singleton()) {
      return split_type_e::invalid;
    }

    if (diff_region_count == 1) {
      return split_type_e::sympatric;
    }

    if (diff_region_count == 0) {
      return split_type_e::allopatric;
    }
  } else if (left_dist.singleton()
             && diff_region_count == 0
             && right_dist == parent_dist) {
    return split_type_e::jump;
  }
  return split_type_e::invalid;
}
\end{minted}
	\caption{A function to determine the split type given three numbers.}
	\label{lst:determine-split-type}
\end{listing}

\subsubsection{Simulating a Phylogenetic Tree}


\subsection{Testing}

\subsubsection{Spread}

For the purposes of ensuring correct results, we perform 2 types of tests.
The first test pre-computes the expected value of the waiting time given the
model parameters, and performs a t-test against that expected value.
We compute 1,886,084,219 iterations of the spread function, which allows us to
be 99.999\% confident that the error is less than 0.0001.
We perform this test for 7 different regions and 16 different parameter sets.

The second test is a regression test against the rejection method.
We again run both methods for 1,886,084,219 iterations, which again ensures
that the error is less than 0.0001 with 99.999\% confidence.
We perform this test for 5 different initial regions and 16 different parameter
sets.

\subsubsection{Split}

To test that the correct proportion of splits are generated by the simulation,
we simulate a sample of splits, and check that the proportion of generated
splits matches the expected proportion.
Specifically, we check that if there are \(n\) simulations for a given model
and range \( r \), then we should expect that there are \(\overline{w_t} \times
n\) splits of type \( t \).
We model the realized count of split types as being drawn from a normal
distribution, and perform a t-test to ensure that the realized count is
approximately equal to the expected count.
Again, we compute 1,886,084,219 simulations for 10 different ranges and 8
different sets of model parameters, which ensures a 99.999\% confidence that
the error is less than 0.0001.

Like with the spread, we use the slower rejection method to test that the
results of the optimized method are correct.
We run both methods for 487,791,396 iterations and count of generated types for
each.
This number of iterations ensures that the methods produce the same results to
a tolerance of \texttt{1e-4} with 99.999\% confidence.
We perform this test for 5 different parent regions, and 5 different sets of
model parameters.

\section{Results}

\begin{figure}
    \centering
    \includesvg[width=\linewidth, pretex=\relscale{0.5}]{figs/bigrig.times.linreg.svg}
    \caption{Runtime plot}\label{fig:runtime-taxa}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=\linewidth, pretex=\relscale{0.5}]{figs/bigrig.times.boxplot.svg}
    \caption{Runtime plot}\label{fig:runtime-regions}
\end{figure}

\section{Conclusion}

\section{Acknowledgements}

Many thanks to Alexander I. Jordan for his assistance on deriving and
implementing the statistical tests used to verify the results of \bigrig{}.

\bibliography{references}

\end{document}
